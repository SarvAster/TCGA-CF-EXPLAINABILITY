{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DICE algorithm\n",
    "Input : parquet file with classification value 1 as first column, and feature values \n",
    "Output : list of valid counterfactuals, with classification value 0 at first column, \n",
    "    and with a new last column with the original factual row index\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2000)\n",
    "        self.bn1 = nn.BatchNorm1d(2000)\n",
    "        self.dropout1 = nn.Dropout(0.02)\n",
    "        self.fc2 = nn.Linear(2000, 200)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.dropout2 = nn.Dropout(0.02)\n",
    "        self.fc3 = nn.Linear(200, 20)\n",
    "        self.bn3 = nn.BatchNorm1d(20)\n",
    "        self.dropout3 = nn.Dropout(0.02)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "def load_model(model_path, input_size, device):\n",
    "    \"\"\"Load the pretrained MLP model\"\"\"\n",
    "    model = MLP(input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def hinge_loss(predictions, desired_class=0):\n",
    "    \"\"\"\n",
    "    Compute hinge loss for counterfactual generation\n",
    "    Ensures zero penalty when predictions are already in the desired class\n",
    "    \"\"\"\n",
    "    # For desired class 0, we want predictions to be below 0.5\n",
    "    if desired_class == 0:\n",
    "        # Compute logits (inverse of sigmoid)\n",
    "        logits = -torch.log(1/predictions - 1)\n",
    "        # Returns zero when prediction is already below 0.5 (logit < 0)\n",
    "        return torch.max(torch.zeros_like(logits), 1 + logits)\n",
    "    else:\n",
    "        # Compute logits\n",
    "        logits = -torch.log(1/predictions - 1)\n",
    "        # Returns zero when prediction is already above 0.5 (logit > 0)\n",
    "        return torch.max(torch.zeros_like(logits), 1 - logits)\n",
    "\n",
    "def compute_l1_distance(x1, x2):\n",
    "    \"\"\"Compute L1 distance between two tensors\"\"\"\n",
    "    return torch.abs(x1 - x2).sum(dim=1, keepdim=True)\n",
    "\n",
    "def compute_dpp_diversity(counterfactuals, epsilon=1e-6):\n",
    "    \"\"\"\n",
    "    Compute determinantal point process diversity\n",
    "    K_ij = 1 / (1 + dist(ci, cj)) where dist is L1 distance\n",
    "    \"\"\"\n",
    "    batch_size, num_cfs, num_features = counterfactuals.shape\n",
    "    \n",
    "    # Compute pairwise L1 distances for all counterfactuals in batch\n",
    "    # Reshape for broadcasting\n",
    "    cfs_expanded1 = counterfactuals.view(batch_size, num_cfs, 1, num_features)\n",
    "    cfs_expanded2 = counterfactuals.view(batch_size, 1, num_cfs, num_features)\n",
    "    \n",
    "    # Calculate L1 distances\n",
    "    pairwise_distances = torch.abs(cfs_expanded1 - cfs_expanded2).sum(dim=3)\n",
    "    \n",
    "    # Compute kernel matrix K\n",
    "    K = 1.0 / (1.0 + pairwise_distances)\n",
    "    \n",
    "    # Add small noise to diagonal to avoid ill-conditioned matrices\n",
    "    diag_noise = torch.randn(batch_size, num_cfs, device=K.device) * epsilon\n",
    "    K = K + torch.diag_embed(diag_noise)\n",
    "    \n",
    "    # Compute determinant for each matrix in the batch\n",
    "    dpp_diversity = torch.linalg.det(K)\n",
    "    \n",
    "    return dpp_diversity\n",
    "\n",
    "def generate_counterfactuals(\n",
    "    data_path,\n",
    "    model_path,\n",
    "    proximity_weight=0.5,\n",
    "    diversity_weight=1.0,\n",
    "    gpu_id=0,\n",
    "    num_cfs=5,\n",
    "    batch_size=32,\n",
    "    learning_rate=0.01,\n",
    "    max_iterations=1000,\n",
    "    early_stop_threshold=0.001,\n",
    "    output_path='counterfactuals.parquet'\n",
    "):\n",
    "    \"\"\"\n",
    "    Generate counterfactual explanations for samples in the dataset\n",
    "    \n",
    "    Parameters:\n",
    "    - data_path: Path to input DataFrame (parquet format)\n",
    "    - model_path: Path to pretrained MLP model\n",
    "    - proximity_weight: Weight for proximity loss (λ1)\n",
    "    - diversity_weight: Weight for diversity loss (λ2)\n",
    "    - gpu_id: GPU device ID to use\n",
    "    - num_cfs: Number of counterfactuals to generate per sample\n",
    "    - batch_size: Batch size for processing\n",
    "    - learning_rate: Learning rate for optimization\n",
    "    - max_iterations: Maximum number of optimization iterations\n",
    "    - early_stop_threshold: Threshold for early stopping\n",
    "    - output_path: Path to save generated counterfactuals\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set up GPU\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df = pd.read_parquet(data_path)\n",
    "    \n",
    "    # Extract features and labels\n",
    "    y = df.iloc[:, 0].values  # First column is the label\n",
    "    X = df.iloc[:, 1:].values  # Remaining columns are features\n",
    "    \n",
    "    # Save the original DataFrame indices for later mapping\n",
    "    original_indices = df.index.tolist()\n",
    "    \n",
    "    # Convert to tensors\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = TensorDataset(X_tensor, y_tensor)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Load model\n",
    "    input_size = X.shape[1]\n",
    "    model = load_model(model_path, input_size, device)\n",
    "    \n",
    "    # Initialize list to store all results\n",
    "    all_results = []\n",
    "    \n",
    "    print(f\"Generating {num_cfs} counterfactuals per sample in batches of {batch_size}...\")\n",
    "    \n",
    "    # Process batches\n",
    "    for batch_idx, (X_batch, _) in enumerate(tqdm(dataloader)):\n",
    "        batch_start_idx = batch_idx * batch_size\n",
    "        batch_size_actual = X_batch.size(0)  # Actual batch size (may be smaller for last batch)\n",
    "        \n",
    "        # Move batch to device\n",
    "        X_batch = X_batch.to(device)\n",
    "        \n",
    "        # Generate counterfactuals for the batch\n",
    "        counterfactuals = optimize_counterfactuals(\n",
    "            model=model,\n",
    "            original_samples=X_batch,\n",
    "            num_cfs=num_cfs,\n",
    "            proximity_weight=proximity_weight,\n",
    "            diversity_weight=diversity_weight,\n",
    "            learning_rate=learning_rate,\n",
    "            max_iterations=max_iterations,\n",
    "            early_stop_threshold=early_stop_threshold,\n",
    "            device=device\n",
    "        )\n",
    "        \n",
    "        # Move results back to CPU for storage\n",
    "        counterfactuals = counterfactuals.detach().cpu()\n",
    "        \n",
    "        # Evaluate model predictions on counterfactuals\n",
    "        with torch.no_grad():\n",
    "            cf_flat = counterfactuals.reshape(-1, input_size)\n",
    "            model.eval()\n",
    "            predictions = model(cf_flat.to(device)).reshape(batch_size_actual, num_cfs).cpu()\n",
    "        \n",
    "        # Store counterfactuals and their metadata\n",
    "        for i in range(batch_size_actual):\n",
    "            # Get the actual DataFrame index for this sample\n",
    "            position_idx = batch_start_idx + i\n",
    "            if position_idx < len(original_indices):\n",
    "                df_index = original_indices[position_idx]\n",
    "                \n",
    "                for j in range(num_cfs):\n",
    "                    # Get prediction for this counterfactual\n",
    "                    pred = predictions[i, j].item()\n",
    "                    \n",
    "                    # Store only if it's a valid counterfactual (predicted as class 0)\n",
    "                    if pred < 0.5:\n",
    "                        # Store the counterfactual with its target class (0) and DataFrame index\n",
    "                        cf_with_metadata = torch.cat([\n",
    "                            torch.tensor([0.0]),  # Target class is 0\n",
    "                            counterfactuals[i, j],\n",
    "                            torch.tensor([float(df_index)])  # DataFrame index (not position)\n",
    "                        ])\n",
    "                        all_results.append(cf_with_metadata.numpy())\n",
    "    \n",
    "    # Convert results to DataFrame and save\n",
    "    if all_results:\n",
    "        result_df = pd.DataFrame(all_results)\n",
    "        result_df.columns = ['class'] + list(df.columns[1:]) + ['original_index']\n",
    "        result_df.to_parquet(output_path)\n",
    "        print(f\"Generated {len(result_df)} counterfactuals saved to {output_path}\")\n",
    "    else:\n",
    "        print(\"No counterfactuals were successfully generated.\")\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Total execution time: {elapsed_time:.2f} seconds\")\n",
    "    \n",
    "    return result_df if all_results else None\n",
    "\n",
    "def optimize_counterfactuals(\n",
    "    model,\n",
    "    original_samples,\n",
    "    num_cfs=5,\n",
    "    proximity_weight=0.5,\n",
    "    diversity_weight=1.0,\n",
    "    learning_rate=0.01,\n",
    "    max_iterations=1000,\n",
    "    early_stop_threshold=0.001,\n",
    "    device=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Optimize counterfactuals for a batch of samples\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The MLP model\n",
    "    - original_samples: Batch of original samples\n",
    "    - num_cfs: Number of counterfactuals to generate per sample\n",
    "    - proximity_weight: Weight for proximity loss (λ1)\n",
    "    - diversity_weight: Weight for diversity loss (λ2)\n",
    "    - learning_rate: Learning rate for optimization\n",
    "    - max_iterations: Maximum number of optimization iterations\n",
    "    - early_stop_threshold: Threshold for early stopping\n",
    "    - device: Device to use for computation\n",
    "    \n",
    "    Returns:\n",
    "    - counterfactuals: Tensor of shape (batch_size, num_cfs, num_features)\n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = original_samples.device\n",
    "    \n",
    "    batch_size, num_features = original_samples.shape\n",
    "    \n",
    "    # Initialize counterfactuals with random noise around original samples\n",
    "    counterfactuals = original_samples.unsqueeze(1).repeat(1, num_cfs, 1)\n",
    "    counterfactuals = counterfactuals + torch.randn_like(counterfactuals) * 0.1\n",
    "    counterfactuals = counterfactuals.clamp(0, 1)  # Ensure values are normalized\n",
    "    \n",
    "    # Make counterfactuals trainable\n",
    "    counterfactuals = counterfactuals.clone().detach().requires_grad_(True)\n",
    "    \n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam([counterfactuals], lr=learning_rate)\n",
    "    \n",
    "    # For early stopping\n",
    "    prev_loss = float('inf')\n",
    "    \n",
    "    # Optimization loop\n",
    "    for iteration in range(max_iterations):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Reshape counterfactuals for model input\n",
    "        cf_flat = counterfactuals.reshape(-1, num_features)\n",
    "        \n",
    "        # Get model predictions\n",
    "        predictions = model(cf_flat).reshape(batch_size, num_cfs)\n",
    "        \n",
    "        # Compute y-loss (hinge loss)\n",
    "        y_loss = hinge_loss(predictions, desired_class=0).mean()\n",
    "        \n",
    "        # Compute proximity loss\n",
    "        original_expanded = original_samples.unsqueeze(1).expand_as(counterfactuals)\n",
    "        proximity_loss = compute_l1_distance(counterfactuals, original_expanded).mean()\n",
    "        \n",
    "        # Compute diversity loss\n",
    "        diversity_loss = -compute_dpp_diversity(counterfactuals).mean()\n",
    "        \n",
    "        # Combine losses\n",
    "        total_loss = y_loss + proximity_weight * proximity_loss + diversity_weight * diversity_loss\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Project back to [0, 1] bounds\n",
    "        with torch.no_grad():\n",
    "            counterfactuals.clamp_(0, 1)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if iteration % 50 == 0:\n",
    "            current_loss = total_loss.item()\n",
    "            loss_difference = abs(prev_loss - current_loss)\n",
    "            if loss_difference < early_stop_threshold:\n",
    "                break\n",
    "            prev_loss = current_loss\n",
    "    \n",
    "    return counterfactuals\n",
    "\n",
    "\n",
    "# For n=3 counterfactuals\n",
    "generate_counterfactuals(\n",
    "    data_path='data_eval_norm.parquet',\n",
    "    model_path='mlp_model.pth',\n",
    "    proximity_weight=0.01,\n",
    "    diversity_weight=1,\n",
    "    gpu_id=0,\n",
    "    num_cfs=3,\n",
    "    batch_size=1024,\n",
    "    output_path='3cf_p001.parquet'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DICE post-hoc sparsity algorithm, adapted with percentile and gradient method, and ranking reversion by ascending or descending values\n",
    "Input : Output of previous script\n",
    "Output : Same format but sparser\"\"\"\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 2000)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(2000)\n",
    "        self.dropout1 = torch.nn.Dropout(0.02)\n",
    "        self.fc2 = torch.nn.Linear(2000, 200)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(200)\n",
    "        self.dropout2 = torch.nn.Dropout(0.02)\n",
    "        self.fc3 = torch.nn.Linear(200, 20)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(20)\n",
    "        self.dropout3 = torch.nn.Dropout(0.02)\n",
    "        self.fc4 = torch.nn.Linear(20, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Handle batch norm for single samples if needed\n",
    "        if x.dim() == 2 and x.size(0) == 1:\n",
    "            # For a single sample, clone it to make a batch of 2\n",
    "            x_batch = torch.cat([x, x], dim=0)\n",
    "            x_batch = self.leaky_relu(self.bn1(self.fc1(x_batch)))\n",
    "            x_batch = self.dropout1(x_batch)\n",
    "            x_batch = self.leaky_relu(self.bn2(self.fc2(x_batch)))\n",
    "            x_batch = self.dropout2(x_batch)\n",
    "            x_batch = self.leaky_relu(self.bn3(self.fc3(x_batch)))\n",
    "            x_batch = self.dropout3(x_batch)\n",
    "            x_batch = self.sigmoid(self.fc4(x_batch))\n",
    "            return x_batch[0:1]  # Return only the first sample\n",
    "        else:\n",
    "            # Normal batch processing\n",
    "            x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout1(x)\n",
    "            x = self.leaky_relu(self.bn2(self.fc2(x)))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.leaky_relu(self.bn3(self.fc3(x)))\n",
    "            x = self.dropout3(x)\n",
    "            x = self.sigmoid(self.fc4(x))\n",
    "            return x\n",
    "\n",
    "def load_model(model_path, input_size, device):\n",
    "    \"\"\"Load the pretrained MLP model\"\"\"\n",
    "    model = MLP(input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def compute_feature_statistics(data_tensor):\n",
    "    \"\"\"\n",
    "    Compute MAD statistics for all features directly on GPU when possible\n",
    "    \n",
    "    Args:\n",
    "        data_tensor: Original data as a PyTorch tensor (without class column)\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (medians, MADs) for each feature\n",
    "    \"\"\"\n",
    "    num_features = data_tensor.shape[1]\n",
    "    \n",
    "    try:\n",
    "        # Try to compute statistics directly on GPU\n",
    "        medians = torch.median(data_tensor, dim=0)[0]\n",
    "        \n",
    "        # Compute deviations\n",
    "        deviations = torch.abs(data_tensor - medians.unsqueeze(0))\n",
    "        \n",
    "        # Compute MAD for each feature\n",
    "        mads = torch.median(deviations, dim=0)[0]\n",
    "        \n",
    "        # Replace zeros with small value to avoid division by zero\n",
    "        mads = torch.clamp(mads, min=1e-6)\n",
    "        \n",
    "        return medians, mads\n",
    "        \n",
    "    except RuntimeError:\n",
    "        # Fallback to CPU if GPU memory is exceeded\n",
    "        print(\"WARNING: Falling back to CPU for statistics computation\")\n",
    "        data_np = data_tensor.cpu().numpy()\n",
    "        medians = np.zeros(num_features)\n",
    "        mads = np.zeros(num_features)\n",
    "        \n",
    "        for feature_idx in range(num_features):\n",
    "            values = data_np[:, feature_idx]\n",
    "            \n",
    "            # Compute median absolute deviation (MAD)\n",
    "            median = np.median(values)\n",
    "            deviations = np.abs(values - median)\n",
    "            mad = np.median(deviations)\n",
    "            \n",
    "            medians[feature_idx] = median\n",
    "            mads[feature_idx] = mad if mad > 0 else 1e-6  # Avoid division by zero\n",
    "        \n",
    "        return (\n",
    "            torch.tensor(medians, dtype=data_tensor.dtype, device=data_tensor.device),\n",
    "            torch.tensor(mads, dtype=data_tensor.dtype, device=data_tensor.device)\n",
    "        )\n",
    "\n",
    "def compute_percentile_thresholds(data_tensor, percentile_param):\n",
    "    \"\"\"\n",
    "    Compute percentile statistics for all features directly on GPU when possible\n",
    "    \n",
    "    Args:\n",
    "        data_tensor: Original data as a PyTorch tensor (without class column)\n",
    "        percentile_param: Percentile threshold (e.g., 0.1 for 10th percentile)\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of percentile thresholds for each feature\n",
    "    \"\"\"\n",
    "    num_features = data_tensor.shape[1]\n",
    "    \n",
    "    try:\n",
    "        # Try using torch operations on GPU\n",
    "        # Compute medians on GPU\n",
    "        medians = torch.median(data_tensor, dim=0)[0]\n",
    "        \n",
    "        # Compute deviations\n",
    "        deviations = torch.abs(data_tensor - medians.unsqueeze(0))\n",
    "        \n",
    "        # Initialize percentiles tensor\n",
    "        percentiles = torch.full((num_features,), 1e-6, \n",
    "                               dtype=data_tensor.dtype, \n",
    "                               device=data_tensor.device)\n",
    "        \n",
    "        # Compute percentiles for each feature\n",
    "        for feature_idx in range(num_features):\n",
    "            feature_devs = deviations[:, feature_idx]\n",
    "            non_zero_mask = feature_devs > 0\n",
    "            \n",
    "            if non_zero_mask.sum() > 0:\n",
    "                # Get non-zero deviations\n",
    "                non_zero_devs = feature_devs[non_zero_mask]\n",
    "                \n",
    "                # Use torch.quantile for percentile calculation\n",
    "                percentiles[feature_idx] = torch.quantile(\n",
    "                    non_zero_devs, \n",
    "                    percentile_param, \n",
    "                    interpolation='linear'\n",
    "                )\n",
    "        \n",
    "        return percentiles\n",
    "        \n",
    "    except (RuntimeError, AttributeError):\n",
    "        # Fallback to CPU if GPU memory is exceeded or torch version doesn't support quantile\n",
    "        print(\"WARNING: Falling back to CPU for percentile computation\")\n",
    "        data_np = data_tensor.cpu().numpy()\n",
    "        percentiles = np.zeros(num_features)\n",
    "        \n",
    "        for feature_idx in range(num_features):\n",
    "            values = data_np[:, feature_idx]\n",
    "            \n",
    "            # Compute median absolute deviation (MAD)\n",
    "            median = np.median(values)\n",
    "            deviations = np.abs(values - median)\n",
    "            \n",
    "            # Compute percentile of non-zero deviations\n",
    "            non_zero_deviations = deviations[deviations > 0]\n",
    "            if len(non_zero_deviations) > 0:\n",
    "                percentiles[feature_idx] = np.percentile(non_zero_deviations, percentile_param * 100)\n",
    "            else:\n",
    "                percentiles[feature_idx] = 1e-6  # Small non-zero value\n",
    "        \n",
    "        return torch.tensor(percentiles, dtype=data_tensor.dtype, device=data_tensor.device)\n",
    "\n",
    "def compute_gradient_importance(model, cf_features, original_features, mads=None, use_mad_norm=False):\n",
    "    \"\"\"\n",
    "    Compute gradient-based importance scores for all features\n",
    "    \n",
    "    Args:\n",
    "        model: PyTorch model\n",
    "        cf_features: Counterfactual features tensor\n",
    "        original_features: Original features tensor\n",
    "        mads: Median absolute deviations (optional, for normalization)\n",
    "        use_mad_norm: Whether to normalize by MAD\n",
    "        \n",
    "    Returns:\n",
    "        Tensor of importance scores (lower = more mutable)\n",
    "    \"\"\"\n",
    "    # Create a copy for gradient computation to avoid modifying the input\n",
    "    with torch.enable_grad():\n",
    "        cf_features_grad = cf_features.clone().requires_grad_(True)\n",
    "        \n",
    "        # Forward pass\n",
    "        output = model(cf_features_grad)\n",
    "        \n",
    "        # Compute gradients\n",
    "        output.sum().backward()\n",
    "        \n",
    "        # Calculate importance scores\n",
    "        # Importance = gradient * |cf - original|\n",
    "        gradients = cf_features_grad.grad.abs().detach()\n",
    "    \n",
    "    differences = torch.abs(cf_features - original_features)\n",
    "    importance = gradients * differences\n",
    "    \n",
    "    # Normalize by MAD if requested\n",
    "    if use_mad_norm and mads is not None:\n",
    "        importance = importance / mads.unsqueeze(0)\n",
    "    \n",
    "    return importance\n",
    "\n",
    "@torch.no_grad()\n",
    "def enhance_sparsity(\n",
    "    original_data_path,\n",
    "    counterfactuals_path,\n",
    "    model_path,\n",
    "    sparsity_param=0.1,\n",
    "    method=\"percentile\",\n",
    "    sort_descending=False,  # New parameter to control sorting direction\n",
    "    gpu_id=0,\n",
    "    output_path='sparse_counterfactuals.parquet',\n",
    "    max_gpu_memory_fraction=0.8\n",
    "):\n",
    "    \"\"\"\n",
    "    Enhance sparsity of counterfactuals by modifying features according to specified method\n",
    "    \n",
    "    Args:\n",
    "        original_data_path: Path to original normalized data\n",
    "        counterfactuals_path: Path to generated counterfactuals\n",
    "        model_path: Path to trained MLP model\n",
    "        sparsity_param: Parameter controlling sparsity enhancement:\n",
    "            - If method='percentile': percentile threshold (0.1 = 10th percentile)\n",
    "            - If method='gradient': sparsity_param > 0.5 uses MAD normalization\n",
    "        method: Feature ranking method ('percentile' or 'gradient')\n",
    "        sort_descending: Whether to sort features in descending order (True) or ascending (False)\n",
    "        gpu_id: GPU device ID to use\n",
    "        output_path: Path to save sparsity-enhanced counterfactuals\n",
    "        max_gpu_memory_fraction: Maximum fraction of GPU memory to use for tensor storage\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with sparsity-enhanced counterfactuals\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    original_data = pd.read_parquet(original_data_path)\n",
    "    counterfactuals = pd.read_parquet(counterfactuals_path)\n",
    "    \n",
    "    # Check GPU memory to determine if we need to split the data\n",
    "    if torch.cuda.is_available():\n",
    "        total_gpu_memory = torch.cuda.get_device_properties(gpu_id).total_memory\n",
    "        available_gpu_memory = total_gpu_memory * max_gpu_memory_fraction\n",
    "        print(f\"Using up to {available_gpu_memory / 1024**3:.2f} GB of GPU memory\")\n",
    "    else:\n",
    "        available_gpu_memory = float('inf')  # No GPU limit\n",
    "    \n",
    "    # Extract feature names (excluding class and original_index columns)\n",
    "    cf_columns = list(counterfactuals.columns)\n",
    "    feature_names = cf_columns[1:-1]  # Exclude first (class) and last (original_index) columns\n",
    "    num_features = len(feature_names)\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_path, num_features, device)\n",
    "    \n",
    "    # Calculate memory requirements - more conservative estimation due to vectorization\n",
    "    total_counterfactuals = len(counterfactuals)\n",
    "    memory_per_sample = num_features * 4 * 7  # 7 tensors with 4 bytes per float32\n",
    "    total_memory_needed = total_counterfactuals * memory_per_sample\n",
    "    \n",
    "    # Determine batch size based on GPU memory\n",
    "    if total_memory_needed > available_gpu_memory:\n",
    "        batch_size = int(available_gpu_memory / memory_per_sample)\n",
    "        print(f\"Processing in batches of {batch_size} samples due to GPU memory constraints\")\n",
    "        use_batching = True\n",
    "    else:\n",
    "        batch_size = total_counterfactuals\n",
    "        use_batching = False\n",
    "        print(\"Processing all counterfactuals in a single batch\")\n",
    "    \n",
    "    # Parse method parameters\n",
    "    use_mad_norm = False\n",
    "    if method == \"gradient\" or method == \"grad\":  # Added \"grad\" as an acceptable value\n",
    "        if sparsity_param > 0.5:\n",
    "            use_mad_norm = True\n",
    "            print(f\"Using gradient method with MAD normalization\")\n",
    "        else:\n",
    "            print(f\"Using gradient method without MAD normalization\")\n",
    "    else:  # Default to percentile method\n",
    "        method = \"percentile\"\n",
    "        print(f\"Using percentile method with parameter {sparsity_param}\")\n",
    "    \n",
    "    # Process data in batches if needed\n",
    "    sparse_counterfactuals = counterfactuals.copy()\n",
    "    \n",
    "    # Pre-compute statistics for all original data if it fits in memory\n",
    "    if not use_batching and method == \"percentile\":\n",
    "        print(\"Pre-computing statistics for all data...\")\n",
    "        all_original_features = torch.tensor(\n",
    "            original_data[feature_names].values, \n",
    "            dtype=torch.float32, \n",
    "            device=device\n",
    "        )\n",
    "        medians, mads = compute_feature_statistics(all_original_features)\n",
    "        percentiles = compute_percentile_thresholds(all_original_features, sparsity_param)\n",
    "        all_thresholds = torch.minimum(mads, percentiles)\n",
    "        # Use the sort_descending parameter here for global sorting\n",
    "        _, global_sorted_indices = torch.sort(all_thresholds, descending=sort_descending)\n",
    "        \n",
    "        # Free memory\n",
    "        del all_original_features\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Main batch processing loop\n",
    "    for batch_start in tqdm(range(0, total_counterfactuals, batch_size)):\n",
    "        batch_end = min(batch_start + batch_size, total_counterfactuals)\n",
    "        batch_cf = counterfactuals.iloc[batch_start:batch_end]\n",
    "        \n",
    "        # Get original indices and find corresponding original samples\n",
    "        original_indices = batch_cf['original_index'].astype(int).values\n",
    "        original_samples = original_data.iloc[original_indices]\n",
    "        \n",
    "        # Extract features as tensors\n",
    "        cf_features = torch.tensor(batch_cf[feature_names].values, dtype=torch.float32, device=device)\n",
    "        original_features = torch.tensor(original_samples[feature_names].values, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Check initial predictions (we want to keep class = 0)\n",
    "        initial_preds = model(cf_features).view(-1)\n",
    "        valid_cf_mask = initial_preds < 0.5\n",
    "        \n",
    "        # Skip if no valid counterfactuals\n",
    "        if not valid_cf_mask.any():\n",
    "            continue\n",
    "        \n",
    "        # Make a working copy of the counterfactuals to update\n",
    "        working_cf_features = cf_features.clone()\n",
    "        \n",
    "        # Get active samples mask (initially all valid counterfactuals)\n",
    "        active_samples_mask = valid_cf_mask.clone()\n",
    "        \n",
    "        # Compute feature ranking if not pre-computed\n",
    "        if use_batching or method != \"percentile\":\n",
    "            # Compute feature statistics for this batch\n",
    "            medians, mads = compute_feature_statistics(original_features)\n",
    "            \n",
    "            if method == \"percentile\":\n",
    "                # Compute percentile thresholds\n",
    "                percentiles = compute_percentile_thresholds(original_features, sparsity_param)\n",
    "                # Use min(MAD, percentile) as thresholds\n",
    "                thresholds = torch.minimum(mads, percentiles)\n",
    "                # Sort features by thresholds using the sort_descending parameter\n",
    "                _, sorted_indices = torch.sort(thresholds, descending=sort_descending)\n",
    "            else:  # method == \"gradient\" or method == \"grad\"\n",
    "                # Compute gradient-based importance scores\n",
    "                with torch.enable_grad():\n",
    "                    cf_features_grad = cf_features.clone().requires_grad_(True)\n",
    "                    output = model(cf_features_grad)\n",
    "                    output.sum().backward()\n",
    "                    gradients = cf_features_grad.grad.abs()\n",
    "                \n",
    "                # Calculate importance\n",
    "                differences = torch.abs(cf_features - original_features)\n",
    "                importance = gradients * differences\n",
    "                \n",
    "                # Normalize by MAD if requested\n",
    "                if use_mad_norm:\n",
    "                    importance = importance / mads.unsqueeze(0)\n",
    "                \n",
    "                # Average importance across samples\n",
    "                avg_importance = importance.mean(dim=0)\n",
    "                # Sort features by importance using the sort_descending parameter\n",
    "                _, sorted_indices = torch.sort(avg_importance, descending=sort_descending)\n",
    "        else:\n",
    "            # Use pre-computed global indices for percentile method\n",
    "            sorted_indices = global_sorted_indices\n",
    "        \n",
    "        # Process features in sorted order - VECTORIZED APPROACH\n",
    "        for feature_idx in sorted_indices:\n",
    "            # Skip if no active samples\n",
    "            if not active_samples_mask.any():\n",
    "                break\n",
    "                \n",
    "            # Store current values for the specific feature across active samples\n",
    "            current_feature_values = working_cf_features[active_samples_mask, feature_idx].clone()\n",
    "            \n",
    "            # Tentatively revert this feature to original values for all active samples\n",
    "            working_cf_features[active_samples_mask, feature_idx] = original_features[active_samples_mask, feature_idx]\n",
    "            \n",
    "            # Check which samples remain valid counterfactuals in a single batch\n",
    "            batch_preds = model(working_cf_features[active_samples_mask]).view(-1)\n",
    "            \n",
    "            # Find which samples flipped to class 1 (invalid)\n",
    "            flipped_mask = batch_preds >= 0.5\n",
    "            \n",
    "            # If any samples flipped, revert just those changes\n",
    "            if flipped_mask.any():\n",
    "                # Find the original indices of flipped samples within the active set\n",
    "                flipped_indices = torch.where(active_samples_mask)[0][flipped_mask]\n",
    "                \n",
    "                # Revert the change for samples that flipped\n",
    "                working_cf_features[flipped_indices, feature_idx] = current_feature_values[flipped_mask]\n",
    "        \n",
    "        # Update the dataframe with the sparsified counterfactuals\n",
    "        sparse_counterfactuals.iloc[batch_start:batch_end, 1:-1] = working_cf_features.cpu().numpy()\n",
    "    \n",
    "    # Save the sparse counterfactuals\n",
    "    sparse_counterfactuals.to_parquet(output_path)\n",
    "    \n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Total execution time: {elapsed_time:.2f} seconds\")\n",
    "    print(f\"Enhanced counterfactuals saved to {output_path}\")\n",
    "    \n",
    "    return sparse_counterfactuals\n",
    "\n",
    "\n",
    "enhance_sparsity(\n",
    "    original_data_path='data_norm.parquet',\n",
    "    counterfactuals_path='3cf_p100.parquet',\n",
    "    model_path='mlp_model.pth',\n",
    "    sparsity_param=0.1,\n",
    "    method=\"percentile\",\n",
    "    sort_descending=True,\n",
    "    gpu_id=0,\n",
    "    output_path='sparse_3cf_p100_desc.parquet',\n",
    "    max_gpu_memory_fraction=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Evaluation script for validity, sparsity, proximity, diversity, 10-NN distance to average, plausibility\n",
    "Intput : Output dataframe of DICE or post-hoc sparsity algorithm\n",
    "Output : JSON dictionnary with min, average, max values and std for the dataframe\"\"\"\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_size, 2000)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(2000)\n",
    "        self.dropout1 = torch.nn.Dropout(0.02)\n",
    "        self.fc2 = torch.nn.Linear(2000, 200)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(200)\n",
    "        self.dropout2 = torch.nn.Dropout(0.02)\n",
    "        self.fc3 = torch.nn.Linear(200, 20)\n",
    "        self.bn3 = torch.nn.BatchNorm1d(20)\n",
    "        self.dropout3 = torch.nn.Dropout(0.02)\n",
    "        self.fc4 = torch.nn.Linear(20, 1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.leaky_relu = torch.nn.LeakyReLU(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Handle batch norm for single samples if needed\n",
    "        if x.dim() == 2 and x.size(0) == 1:\n",
    "            # For a single sample, clone it to make a batch of 2\n",
    "            x_batch = torch.cat([x, x], dim=0)\n",
    "            x_batch = self.leaky_relu(self.bn1(self.fc1(x_batch)))\n",
    "            x_batch = self.dropout1(x_batch)\n",
    "            x_batch = self.leaky_relu(self.bn2(self.fc2(x_batch)))\n",
    "            x_batch = self.dropout2(x_batch)\n",
    "            x_batch = self.leaky_relu(self.bn3(self.fc3(x_batch)))\n",
    "            x_batch = self.dropout3(x_batch)\n",
    "            x_batch = self.sigmoid(self.fc4(x_batch))\n",
    "            return x_batch[0:1]  # Return only the first sample\n",
    "        else:\n",
    "            # Normal batch processing\n",
    "            x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
    "            x = self.dropout1(x)\n",
    "            x = self.leaky_relu(self.bn2(self.fc2(x)))\n",
    "            x = self.dropout2(x)\n",
    "            x = self.leaky_relu(self.bn3(self.fc3(x)))\n",
    "            x = self.dropout3(x)\n",
    "            x = self.sigmoid(self.fc4(x))\n",
    "            return x\n",
    "        \n",
    "    def find_boundary_intersection(self, x_factual, x_counterfactual, eps=1e-5, max_iterations=50):\n",
    "        \"\"\"\n",
    "        Find the point on the decision boundary that intersects the line between factual and counterfactual\n",
    "        \n",
    "        Args:\n",
    "            x_factual: Factual sample tensor\n",
    "            x_counterfactual: Counterfactual sample tensor\n",
    "            eps: Precision threshold\n",
    "            max_iterations: Maximum number of binary search iterations\n",
    "            \n",
    "        Returns:\n",
    "            Intersection point and distance from factual to intersection\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            # Get predictions for both points\n",
    "            fact_pred = self.forward(x_factual).item()\n",
    "            cf_pred = self.forward(x_counterfactual).item()\n",
    "            \n",
    "            # Check if predictions are different enough (i.e., on opposite sides of the boundary)\n",
    "            # Use a slightly relaxed condition to handle border cases\n",
    "            if (fact_pred > 0.55 and cf_pred > 0.55) or (fact_pred < 0.45 and cf_pred < 0.45):\n",
    "                # If both are clearly on the same side, use the distance to counterfactual\n",
    "                # This is a reasonable fallback rather than returning infinity\n",
    "                return x_counterfactual, torch.sum(torch.abs(x_counterfactual - x_factual)).item()\n",
    "            \n",
    "            # Direction vector from factual to counterfactual\n",
    "            direction = x_counterfactual - x_factual\n",
    "            \n",
    "            # Get direction magnitude (for normalizing)\n",
    "            direction_mag = torch.norm(direction, p=1).item()\n",
    "            if direction_mag < eps:  # If points are too close\n",
    "                return x_factual, 0.0\n",
    "                \n",
    "            # Initialize binary search\n",
    "            low = 0.0  # factual point\n",
    "            high = 1.0  # counterfactual point\n",
    "            \n",
    "            # Binary search for the decision boundary\n",
    "            for _ in range(max_iterations):\n",
    "                mid = (low + high) / 2.0\n",
    "                x_mid = x_factual + mid * direction\n",
    "                pred_mid = self.forward(x_mid).item()\n",
    "                \n",
    "                # Check if we're close enough to the boundary\n",
    "                if abs(pred_mid - 0.5) < eps:\n",
    "                    # Calculate L1 distance from factual to intersection\n",
    "                    distance = torch.sum(torch.abs(x_mid - x_factual)).item()\n",
    "                    return x_mid, distance\n",
    "                \n",
    "                # Update search range\n",
    "                if (pred_mid > 0.5 and fact_pred > 0.5) or (pred_mid < 0.5 and fact_pred < 0.5):\n",
    "                    # Mid point is on same side as factual\n",
    "                    low = mid\n",
    "                else:\n",
    "                    # Mid point is on same side as counterfactual\n",
    "                    high = mid\n",
    "                    \n",
    "                # Check if our search range is small enough\n",
    "                if high - low < eps:\n",
    "                    x_intersection = x_factual + mid * direction\n",
    "                    distance = torch.sum(torch.abs(x_intersection - x_factual)).item()\n",
    "                    return x_intersection, distance\n",
    "            \n",
    "            # If we reach max iterations, return the midpoint\n",
    "            x_intersection = x_factual + ((low + high) / 2.0) * direction\n",
    "            distance = torch.sum(torch.abs(x_intersection - x_factual)).item()\n",
    "            return x_intersection, distance\n",
    "\n",
    "def load_model(model_path, input_size, device):\n",
    "    \"\"\"Load the pretrained MLP model\"\"\"\n",
    "    model = MLP(input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def compute_median_absolute_deviation(data_tensor):\n",
    "    \"\"\"Compute median absolute deviation for each feature\"\"\"\n",
    "    median = torch.median(data_tensor, dim=0).values\n",
    "    deviation = torch.abs(data_tensor - median)\n",
    "    mad = torch.median(deviation, dim=0).values\n",
    "    # Replace zero MAD values with 1 to avoid division by zero\n",
    "    mad[mad == 0] = 1.0\n",
    "    return mad\n",
    "\n",
    "def update_evaluation_file(name, metric, value, results, output_file=\"evaluation.txt\"):\n",
    "    \"\"\"Update the evaluation file with the current results\"\"\"\n",
    "    # Store the result - don't write to file yet (will be done at the end)\n",
    "    results[name][metric] = value\n",
    "    \n",
    "    # Print the update\n",
    "    print(f\"Updated results with {name} - {metric}: {value:.4f}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "def compute_gaussian_probability(distance, mean, variance):\n",
    "    \"\"\"\n",
    "    Compute the probability P(abs(x-mean) > abs(distance-mean))\n",
    "    This gives the probability of being further from the mean than this distance\n",
    "    \"\"\"\n",
    "    # Calculate how far we are from the mean\n",
    "    distance_from_mean = abs(distance - mean)\n",
    "    \n",
    "    # Compute the probability of being further from the mean (one-sided)\n",
    "    # This is equivalent to 2 * (1 - CDF(|x-mean|))\n",
    "    z_score = distance_from_mean / math.sqrt(variance)\n",
    "    probability = 2 * (1 - 0.5 * (1 + math.erf(z_score / math.sqrt(2))))\n",
    "    \n",
    "    return probability\n",
    "\n",
    "\n",
    "import json\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_counterfactuals(\n",
    "    original_data_path,\n",
    "    counterfactual_dfs,\n",
    "    model_path,\n",
    "    metrics_to_compute={\n",
    "        'validity': True,\n",
    "        'proximity': True,\n",
    "        'sparsity': True,\n",
    "        'sparsity_count': True,\n",
    "        'diversity': True,\n",
    "        'sparse_diversity': True,\n",
    "        'avg_10nn_distance': True,\n",
    "        'avg_10nn_dataset': True,\n",
    "        'avg_10nn_class0': True\n",
    "    },\n",
    "    batch_size=512,\n",
    "    gpu_id=0,\n",
    "    output_path=\"evaluation_results.json\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Evaluate counterfactuals using multiple metrics\n",
    "    \n",
    "    Args:\n",
    "        original_data_path: Path to original data\n",
    "        counterfactual_dfs: Dictionary mapping names to counterfactual DataFrames or paths\n",
    "        model_path: Path to trained model\n",
    "        metrics_to_compute: Dictionary specifying which metrics to compute\n",
    "        batch_size: Batch size for GPU processing\n",
    "        gpu_id: GPU device ID to use\n",
    "        output_path: Path to output evaluation results as JSON\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary of evaluation results per counterfactual set\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Set device\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load original data\n",
    "    print(\"Loading original data...\")\n",
    "    original_df = pd.read_parquet(original_data_path)\n",
    "    \n",
    "    # Process counterfactual dataframes\n",
    "    cf_data = {}\n",
    "    for name, cf_df_or_path in counterfactual_dfs.items():\n",
    "        if isinstance(cf_df_or_path, str):\n",
    "            cf_data[name] = pd.read_parquet(cf_df_or_path)\n",
    "        else:\n",
    "            cf_data[name] = cf_df_or_path\n",
    "    \n",
    "    # Extract feature names (excluding class column and original_index)\n",
    "    feature_names = list(original_df.columns)[1:]  # Exclude first column (class)\n",
    "    num_features = len(feature_names)\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_path, num_features, device)\n",
    "    \n",
    "    # Prepare data for GPU\n",
    "    # Convert original data to tensor (exclude class column)\n",
    "    original_features = torch.tensor(original_df.iloc[:, 1:].values, dtype=torch.float32, device=device)\n",
    "    \n",
    "    # Compute median absolute deviation (MAD) for normalization\n",
    "    mad = compute_median_absolute_deviation(original_features)\n",
    "    \n",
    "    # Extract class information\n",
    "    original_classes = original_df.iloc[:, 0].values\n",
    "    \n",
    "    # Create dictionary to store results\n",
    "    results = {name: {} for name in cf_data.keys()}\n",
    "    \n",
    "    # Gaussian parameters for new metrics\n",
    "    dataset_gaussian_mean = 8304.722022\n",
    "    dataset_gaussian_variance = 4707926.898804\n",
    "    class0_gaussian_mean = 5950.676427\n",
    "    class0_gaussian_variance = 3400381.504375\n",
    "    \n",
    "    # Pre-compute class 0 mask for the original data\n",
    "    class0_mask = torch.tensor(original_classes == 0, device=device)\n",
    "    class0_samples = original_features[class0_mask] if torch.any(class0_mask) else None\n",
    "    \n",
    "    # Process each counterfactual dataset\n",
    "    for name, cf_df in cf_data.items():\n",
    "        print(f\"\\nEvaluating: {name}\")\n",
    "        \n",
    "        # FIXED: Direct mapping from counterfactual to original row positions\n",
    "        cf_to_orig_map = {}\n",
    "        if 'original_index' in cf_df.columns:\n",
    "            for i, row in enumerate(cf_df.itertuples()):\n",
    "                # original_index directly gives the row position in data_norm\n",
    "                orig_position = int(row.original_index)\n",
    "                if 0 <= orig_position < len(original_df):\n",
    "                    cf_to_orig_map[i] = orig_position\n",
    "                else:\n",
    "                    cf_to_orig_map[i] = -1\n",
    "        else:\n",
    "            # Assume sample_id column refers to row positions as well\n",
    "            for i, row in enumerate(cf_df.itertuples()):\n",
    "                if hasattr(row, 'sample_id'):\n",
    "                    orig_position = int(row.sample_id)\n",
    "                    if 0 <= orig_position < len(original_df):\n",
    "                        cf_to_orig_map[i] = orig_position\n",
    "                    else:\n",
    "                        cf_to_orig_map[i] = -1\n",
    "                else:\n",
    "                    cf_to_orig_map[i] = -1\n",
    "        \n",
    "        # Convert counterfactual features to tensor (exclude class column and original_index/sample_id)\n",
    "        if 'original_index' in cf_df.columns:\n",
    "            cf_features = torch.tensor(cf_df.iloc[:, 1:-1].values, dtype=torch.float32, device=device)\n",
    "        else:\n",
    "            # Assume first column is class and rest are features (no original_index)\n",
    "            cf_features = torch.tensor(cf_df.iloc[:, 1:].values, dtype=torch.float32, device=device)\n",
    "        \n",
    "        # Organize counterfactuals by original index\n",
    "        cf_by_orig = {}\n",
    "        for cf_idx, orig_idx in cf_to_orig_map.items():\n",
    "            if orig_idx != -1:\n",
    "                if orig_idx not in cf_by_orig:\n",
    "                    cf_by_orig[orig_idx] = []\n",
    "                cf_by_orig[orig_idx].append(cf_idx)\n",
    "        \n",
    "        # Store metric values for this counterfactual set\n",
    "        metric_values = {}\n",
    "        \n",
    "        # ------ Compute metrics ------\n",
    "        \n",
    "        # 1. Validity\n",
    "        if metrics_to_compute.get('validity', False):\n",
    "            print(\"Computing validity...\")\n",
    "            validity_scores = []\n",
    "            \n",
    "            # Process in batches\n",
    "            for start_idx in range(0, len(cf_features), batch_size):\n",
    "                end_idx = min(start_idx + batch_size, len(cf_features))\n",
    "                batch_cf = cf_features[start_idx:end_idx]\n",
    "                \n",
    "                # Get original indices for this batch\n",
    "                batch_orig_indices = [cf_to_orig_map.get(i, -1) for i in range(start_idx, end_idx)]\n",
    "                \n",
    "                # Filter out invalid mappings\n",
    "                valid_indices = [(i, orig_idx) for i, orig_idx in enumerate(batch_orig_indices) if orig_idx != -1]\n",
    "                if not valid_indices:\n",
    "                    continue\n",
    "                \n",
    "                # Split into cf indices and orig indices\n",
    "                batch_cf_indices, batch_orig_indices = zip(*valid_indices)\n",
    "                \n",
    "                # Get predictions for valid counterfactuals\n",
    "                valid_cf = batch_cf[list(batch_cf_indices)]\n",
    "                cf_preds = model(valid_cf).cpu().numpy()\n",
    "                \n",
    "                # Get predictions for corresponding original samples\n",
    "                orig_samples = original_features[list(batch_orig_indices)]\n",
    "                orig_preds = model(orig_samples).cpu().numpy()\n",
    "                \n",
    "                # Compute validity for each counterfactual\n",
    "                for cf_pred, orig_pred in zip(cf_preds, orig_preds):\n",
    "                    # Binary label change (0 or 1)\n",
    "                    validity_score = abs(float(cf_pred > 0.5) - float(orig_pred >= 0.5))\n",
    "                    validity_scores.append(validity_score)\n",
    "            \n",
    "            if validity_scores:\n",
    "                min_val = float(np.min(validity_scores))\n",
    "                avg_val = float(np.mean(validity_scores))\n",
    "                max_val = float(np.max(validity_scores))\n",
    "                std_val = float(np.std(validity_scores))\n",
    "                metric_values['validity'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"validity: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "        \n",
    "        # 2-3. Proximity and Sparsity\n",
    "        if metrics_to_compute.get('proximity', False) or metrics_to_compute.get('sparsity', False) or metrics_to_compute.get('sparsity_count', False):\n",
    "            print(\"Computing proximity and sparsity...\")\n",
    "            proximity_scores = []\n",
    "            sparsity_scores = []\n",
    "            sparsity_count_scores = []\n",
    "            \n",
    "            # Process in batches\n",
    "            for start_idx in range(0, len(cf_features), batch_size):\n",
    "                end_idx = min(start_idx + batch_size, len(cf_features))\n",
    "                batch_cf = cf_features[start_idx:end_idx]\n",
    "                \n",
    "                # Get original indices for this batch\n",
    "                batch_orig_indices = [cf_to_orig_map.get(i, -1) for i in range(start_idx, end_idx)]\n",
    "                \n",
    "                # Filter out invalid mappings\n",
    "                valid_indices = [(i, orig_idx) for i, orig_idx in enumerate(batch_orig_indices) if orig_idx != -1]\n",
    "                if not valid_indices:\n",
    "                    continue\n",
    "                \n",
    "                # Split into cf indices and orig indices\n",
    "                batch_cf_indices, batch_orig_indices = zip(*valid_indices)\n",
    "                \n",
    "                # Get valid counterfactuals and corresponding original samples\n",
    "                valid_cf = batch_cf[list(batch_cf_indices)]\n",
    "                orig_samples = original_features[list(batch_orig_indices)]\n",
    "                \n",
    "                # Compute absolute differences\n",
    "                abs_diff = torch.abs(valid_cf - orig_samples)\n",
    "                \n",
    "                # Compute proximity (L1 distance normalized by MAD)\n",
    "                if metrics_to_compute.get('proximity', False):\n",
    "                    normalized_diff = abs_diff / mad\n",
    "                    batch_proximity = torch.mean(normalized_diff, dim=1)\n",
    "                    proximity_scores.extend(batch_proximity.cpu().numpy())\n",
    "                \n",
    "                # Compute sparsity (1 - proportion of changed features)\n",
    "                if metrics_to_compute.get('sparsity', False):\n",
    "                    # Consider a feature changed if abs_diff > 1e-2\n",
    "                    changed_features = (abs_diff > 1e-2).float()\n",
    "                    batch_sparsity = 1 - torch.mean(changed_features, dim=1)\n",
    "                    sparsity_scores.extend(batch_sparsity.cpu().numpy())\n",
    "                    \n",
    "                # Compute sparsity count (raw count of changed features)\n",
    "                if metrics_to_compute.get('sparsity_count', False):\n",
    "                    changed_count = torch.sum((abs_diff > 1e-2).float(), dim=1)\n",
    "                    sparsity_count_scores.extend(changed_count.cpu().numpy())\n",
    "            \n",
    "            if metrics_to_compute.get('proximity', False) and proximity_scores:\n",
    "                min_val = float(np.min(proximity_scores))\n",
    "                avg_val = float(np.mean(proximity_scores))\n",
    "                max_val = float(np.max(proximity_scores))\n",
    "                std_val = float(np.std(proximity_scores))\n",
    "                metric_values['proximity'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"proximity: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "                \n",
    "            if metrics_to_compute.get('sparsity', False) and sparsity_scores:\n",
    "                min_val = float(np.min(sparsity_scores))\n",
    "                avg_val = float(np.mean(sparsity_scores))\n",
    "                max_val = float(np.max(sparsity_scores))\n",
    "                std_val = float(np.std(sparsity_scores))\n",
    "                metric_values['sparsity'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"sparsity: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "                    \n",
    "            if metrics_to_compute.get('sparsity_count', False) and sparsity_count_scores:\n",
    "                min_val = float(np.min(sparsity_count_scores))\n",
    "                avg_val = float(np.mean(sparsity_count_scores))\n",
    "                max_val = float(np.max(sparsity_count_scores))\n",
    "                std_val = float(np.std(sparsity_count_scores))\n",
    "                metric_values['sparsity_count'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"sparsity_count: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "        \n",
    "        # 4. Diversity\n",
    "        if metrics_to_compute.get('diversity', False):\n",
    "            print(\"Computing diversity...\")\n",
    "            diversity_scores = []\n",
    "            \n",
    "            # For each original sample, compute diversity of its counterfactuals\n",
    "            for orig_idx, cf_indices in cf_by_orig.items():\n",
    "                if len(cf_indices) > 1:  # Need at least 2 CFs to compute diversity\n",
    "                    # Get counterfactuals for this original sample\n",
    "                    orig_cfs = cf_features[cf_indices]\n",
    "                    \n",
    "                    # Compute pairwise distances using torch.cdist (fully vectorized)\n",
    "                    # Using raw L1 distances without MAD normalization\n",
    "                    pairwise_distances = torch.cdist(orig_cfs, orig_cfs, p=1)\n",
    "                    \n",
    "                    # Extract the upper triangular part (excluding diagonal)\n",
    "                    mask = torch.triu(torch.ones_like(pairwise_distances), diagonal=1).bool()\n",
    "                    distances = pairwise_distances[mask]\n",
    "                    \n",
    "                    # Compute average distance\n",
    "                    if len(distances) > 0:\n",
    "                        avg_distance = torch.mean(distances).item()\n",
    "                        diversity_scores.append(avg_distance)\n",
    "            \n",
    "            if diversity_scores:\n",
    "                min_val = float(np.min(diversity_scores))\n",
    "                avg_val = float(np.mean(diversity_scores))\n",
    "                max_val = float(np.max(diversity_scores))\n",
    "                std_val = float(np.std(diversity_scores))\n",
    "                metric_values['diversity'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"diversity: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "        \n",
    "        # 5. Sparse Diversity\n",
    "        if metrics_to_compute.get('sparse_diversity', False):\n",
    "            print(\"Computing sparse diversity...\")\n",
    "            sparse_diversity_scores = []\n",
    "            \n",
    "            # For each original sample, compute sparse diversity of its counterfactuals\n",
    "            for orig_idx, cf_indices in cf_by_orig.items():\n",
    "                if len(cf_indices) > 1:  # Need at least 2 CFs to compute diversity\n",
    "                    # Get counterfactuals for this original sample\n",
    "                    orig_cfs = cf_features[cf_indices]\n",
    "                    orig_sample = original_features[orig_idx]\n",
    "                    \n",
    "                    # Identify changed features for each counterfactual (binary mask)\n",
    "                    changed_features = (torch.abs(orig_cfs - orig_sample.unsqueeze(0)) > 1e-5).float()\n",
    "                    \n",
    "                    # Number of counterfactuals\n",
    "                    n_cfs = len(cf_indices)\n",
    "                    \n",
    "                    # Vectorized intersection computation\n",
    "                    # Changed features is [n_cfs, num_features]\n",
    "                    # We'll compute intersection for all pairs at once\n",
    "                    \n",
    "                    # Create expanded tensors for broadcasting\n",
    "                    # [n_cfs, 1, num_features] and [1, n_cfs, num_features]\n",
    "                    features_i = changed_features.unsqueeze(1)\n",
    "                    features_j = changed_features.unsqueeze(0)\n",
    "                    \n",
    "                    # Compute intersection and union for all pairs\n",
    "                    intersection = torch.sum(features_i * features_j, dim=2)  # [n_cfs, n_cfs]\n",
    "                    union = torch.sum(torch.clamp(features_i + features_j, 0, 1), dim=2)  # [n_cfs, n_cfs]\n",
    "                    \n",
    "                    # Create a mask for valid pairs (upper triangle, excluding diagonal)\n",
    "                    mask = torch.triu(torch.ones(n_cfs, n_cfs, device=device), diagonal=1).bool()\n",
    "                    \n",
    "                    # Get valid intersection and union values\n",
    "                    valid_intersection = intersection[mask]\n",
    "                    valid_union = union[mask]\n",
    "                    \n",
    "                    # Compute IoU (avoid division by zero)\n",
    "                    valid_mask = valid_union > 0\n",
    "                    if torch.any(valid_mask):\n",
    "                        iou = torch.zeros_like(valid_intersection)\n",
    "                        iou[valid_mask] = valid_intersection[valid_mask] / valid_union[valid_mask]\n",
    "                        \n",
    "                        # Compute average IoU\n",
    "                        avg_overlap = torch.mean(iou).item()\n",
    "                        sparse_diversity_scores.append(1 - avg_overlap)\n",
    "            \n",
    "            if sparse_diversity_scores:\n",
    "                min_val = float(np.min(sparse_diversity_scores))\n",
    "                avg_val = float(np.mean(sparse_diversity_scores))\n",
    "                max_val = float(np.max(sparse_diversity_scores))\n",
    "                std_val = float(np.std(sparse_diversity_scores))\n",
    "                metric_values['sparse_diversity'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"sparse_diversity: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "        \n",
    "        # 6-8. New Metrics: Average 10-NN Distance and Probabilities\n",
    "        # Initialize storage for distances\n",
    "        avg_10nn_distances_list = []\n",
    "        avg_10nn_dataset_probs = []\n",
    "        avg_10nn_class0_probs = []\n",
    "        \n",
    "        # First compute k-NN distances for each counterfactual\n",
    "        if metrics_to_compute.get('avg_10nn_distance', False) or metrics_to_compute.get('avg_10nn_dataset', False) or metrics_to_compute.get('avg_10nn_class0', False):\n",
    "            print(\"Computing 10-NN distances in dataset and class 0...\")\n",
    "            \n",
    "            # Process counterfactuals in batches\n",
    "            for start_idx in range(0, len(cf_features), batch_size):\n",
    "                end_idx = min(start_idx + batch_size, len(cf_features))\n",
    "                batch_cf = cf_features[start_idx:end_idx]\n",
    "                \n",
    "                # Get mappings for this batch\n",
    "                batch_orig_indices = [cf_to_orig_map.get(i, -1) for i in range(start_idx, end_idx)]\n",
    "                valid_mask = torch.tensor([idx != -1 for idx in batch_orig_indices], device=device)\n",
    "                \n",
    "                if not torch.any(valid_mask):\n",
    "                    continue\n",
    "                \n",
    "                valid_cf = batch_cf[valid_mask]\n",
    "                valid_orig_indices = [idx for idx in batch_orig_indices if idx != -1]\n",
    "                \n",
    "                # Find the 10 nearest neighbors in the entire dataset\n",
    "                if metrics_to_compute.get('avg_10nn_distance', False) or metrics_to_compute.get('avg_10nn_dataset', False):\n",
    "                    # Initialize storage for 10-NN distances\n",
    "                    k = 11  # k+1 to avoid counting self (though unlikely to be exact match)\n",
    "                    knn_distances = torch.full((len(valid_cf), k), float('inf'), device=device)\n",
    "                    \n",
    "                    # Process original data in chunks\n",
    "                    chunk_size = 5000  # Adjust based on available GPU memory\n",
    "                    for chunk_start in range(0, len(original_features), chunk_size):\n",
    "                        chunk_end = min(chunk_start + chunk_size, len(original_features))\n",
    "                        data_chunk = original_features[chunk_start:chunk_end]\n",
    "                        \n",
    "                        # Compute pairwise L1 distances between valid_cf and this chunk\n",
    "                        distances = torch.cdist(valid_cf, data_chunk, p=1)\n",
    "                        \n",
    "                        # Update top-k distances\n",
    "                        combined = torch.cat([knn_distances, distances], dim=1)\n",
    "                        topk_values, _ = torch.topk(combined, k=k, dim=1, largest=False)\n",
    "                        knn_distances = topk_values\n",
    "                    \n",
    "                    # Calculate average 10-NN distance for each counterfactual\n",
    "                    # Skip the first one (potentially self) and take the next 10\n",
    "                    avg_10nn_distances = torch.mean(knn_distances[:, 1:11], dim=1)\n",
    "                    \n",
    "                    # Save raw distances for the avg_10nn_distance metric\n",
    "                    if metrics_to_compute.get('avg_10nn_distance', False):\n",
    "                        avg_10nn_distances_list.extend(avg_10nn_distances.cpu().numpy())\n",
    "                    \n",
    "                    # Compute the Gaussian probability for each distance\n",
    "                    if metrics_to_compute.get('avg_10nn_dataset', False):\n",
    "                        for dist in avg_10nn_distances.cpu().numpy():\n",
    "                            prob = compute_gaussian_probability(dist, dataset_gaussian_mean, dataset_gaussian_variance)\n",
    "                            avg_10nn_dataset_probs.append(prob)\n",
    "                \n",
    "                # Find the 10 nearest neighbors in class 0\n",
    "                if metrics_to_compute.get('avg_10nn_class0', False) and class0_samples is not None and len(class0_samples) > 0:\n",
    "                    # Initialize storage for 10-NN distances\n",
    "                    k = min(11, len(class0_samples))  # k+1 to avoid self, but cap at available samples\n",
    "                    knn_distances = torch.full((len(valid_cf), k), float('inf'), device=device)\n",
    "                    \n",
    "                    # Process class 0 data in chunks\n",
    "                    chunk_size = 5000  # Adjust based on available GPU memory\n",
    "                    for chunk_start in range(0, len(class0_samples), chunk_size):\n",
    "                        chunk_end = min(chunk_start + chunk_size, len(class0_samples))\n",
    "                        data_chunk = class0_samples[chunk_start:chunk_end]\n",
    "                        \n",
    "                        # Compute pairwise L1 distances between valid_cf and this chunk\n",
    "                        distances = torch.cdist(valid_cf, data_chunk, p=1)\n",
    "                        \n",
    "                        # Update top-k distances\n",
    "                        combined = torch.cat([knn_distances, distances], dim=1)\n",
    "                        topk_values, _ = torch.topk(combined, k=k, dim=1, largest=False)\n",
    "                        knn_distances = topk_values\n",
    "                    \n",
    "                    # Calculate average 10-NN distance for each counterfactual\n",
    "                    # Skip the first one (potentially self) and take up to the next 10\n",
    "                    if k > 1:\n",
    "                        # Use as many neighbors as available after skipping the first one\n",
    "                        neighbors_to_use = min(10, k-1)\n",
    "                        avg_10nn_distances = torch.mean(knn_distances[:, 1:1+neighbors_to_use], dim=1)\n",
    "                        \n",
    "                        # Compute the Gaussian probability for each distance\n",
    "                        for dist in avg_10nn_distances.cpu().numpy():\n",
    "                            prob = compute_gaussian_probability(dist, class0_gaussian_mean, class0_gaussian_variance)\n",
    "                            avg_10nn_class0_probs.append(prob)\n",
    "            \n",
    "            # Compute and save metrics with min, avg, max, std\n",
    "            if metrics_to_compute.get('avg_10nn_distance', False) and avg_10nn_distances_list:\n",
    "                min_val = float(np.min(avg_10nn_distances_list))\n",
    "                avg_val = float(np.mean(avg_10nn_distances_list))\n",
    "                max_val = float(np.max(avg_10nn_distances_list))\n",
    "                std_val = float(np.std(avg_10nn_distances_list))\n",
    "                metric_values['avg_10nn_distance'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"avg_10nn_distance: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "            \n",
    "            if metrics_to_compute.get('avg_10nn_dataset', False) and avg_10nn_dataset_probs:\n",
    "                min_val = float(np.min(avg_10nn_dataset_probs))\n",
    "                avg_val = float(np.mean(avg_10nn_dataset_probs))\n",
    "                max_val = float(np.max(avg_10nn_dataset_probs))\n",
    "                std_val = float(np.std(avg_10nn_dataset_probs))\n",
    "                metric_values['avg_10nn_dataset_probability'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"avg_10nn_dataset_probability: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "            \n",
    "            if metrics_to_compute.get('avg_10nn_class0', False) and avg_10nn_class0_probs:\n",
    "                min_val = float(np.min(avg_10nn_class0_probs))\n",
    "                avg_val = float(np.mean(avg_10nn_class0_probs))\n",
    "                max_val = float(np.max(avg_10nn_class0_probs))\n",
    "                std_val = float(np.std(avg_10nn_class0_probs))\n",
    "                metric_values['avg_10nn_class0_probability'] = [min_val, avg_val, max_val, std_val]\n",
    "                print(f\"avg_10nn_class0_probability: min={min_val:.4f}, avg={avg_val:.4f}, max={max_val:.4f}, std={std_val:.4f}\")\n",
    "        \n",
    "        # Store metric values for this counterfactual set\n",
    "        results[name] = metric_values\n",
    "    \n",
    "        # Print summary\n",
    "        print(\"\\nEvaluation Summary:\")\n",
    "        for name, metrics in results.items():\n",
    "            print(f\"\\n{name}:\")\n",
    "            for metric, values in metrics.items():\n",
    "                print(f\"  {metric}: min={values[0]:.4f}, avg={values[1]:.4f}, max={values[2]:.4f}, std={values[3]:.4f}\")\n",
    "\n",
    "        elapsed_time = time.time() - start_time\n",
    "        print(f\"\\nTotal evaluation time: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "    # Write results to JSON file - modified to append to existing file if present\n",
    "    print(f\"\\nSaving results to {output_path}\")\n",
    "    try:\n",
    "        # Try to load existing JSON file\n",
    "        if os.path.exists(output_path):\n",
    "            with open(output_path, 'r') as f:\n",
    "                try:\n",
    "                    existing_results = json.load(f)\n",
    "                    print(f\"Successfully loaded existing results from {output_path}\")\n",
    "\n",
    "                    # Update with new results (append or overwrite)\n",
    "                    for name, metrics in results.items():\n",
    "                        if name in existing_results:\n",
    "                            print(f\"Updating existing entry: {name}\")\n",
    "                        else:\n",
    "                            print(f\"Adding new entry: {name}\")\n",
    "                        existing_results[name] = metrics\n",
    "\n",
    "                    # Save updated results\n",
    "                    results = existing_results\n",
    "                except json.JSONDecodeError:\n",
    "                    print(f\"Could not parse existing file as JSON. Creating new file.\")\n",
    "\n",
    "        # Write the combined results\n",
    "        with open(output_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "\n",
    "        print(f\"Results successfully written to {output_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving results to {output_path}: {str(e)}\")\n",
    "        # Create a backup file in case of error\n",
    "        backup_path = f\"{output_path}.backup\"\n",
    "        with open(backup_path, 'w') as f:\n",
    "            json.dump(results, f, indent=2)\n",
    "        print(f\"Backup saved to {backup_path}\")\n",
    "\n",
    "        return results\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "results = evaluate_counterfactuals(\n",
    "    original_data_path='data_norm.parquet',\n",
    "    counterfactual_dfs = {\n",
    "        'best_counterfactuals_2feat_osvm':'best_counterfactuals_2feat_osvm_round.parquet'\n",
    "    },\n",
    "    model_path='mlp_model.pth',\n",
    "    metrics_to_compute={\n",
    "        'validity': True,\n",
    "        'proximity': True,\n",
    "        'sparsity': True,\n",
    "        'sparsity_count': True,\n",
    "        'diversity': True,\n",
    "        'sparse_diversity': True,\n",
    "        'avg_10nn_distance': True, \n",
    "        'avg_10nn_dataset': True,\n",
    "        'avg_10nn_class0': True\n",
    "    },\n",
    "    batch_size=512,\n",
    "    gpu_id=0,\n",
    "    output_path=\"evaluation_with_knn_metrics.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:0\n",
      "Loading data...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-8f5038a064e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0mprediction_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2048\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0mknn_chunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 509\u001b[0;31m     \u001b[0mgpu_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    510\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-2-8f5038a064e3>\u001b[0m in \u001b[0;36mcompute_scores_with_precomputed_predictions\u001b[0;34m(tcga_class0_path, data_eval_path, model_path, num_points, range_percent, diff_weight, nn_weight, l1_weight, gpu_id, batch_size, prediction_batch_size, knn_chunk_size, precompute_save_path, output_cf_path, output_importance_path)\u001b[0m\n\u001b[1;32m    306\u001b[0m     \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading data...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m     \u001b[0mdf_class0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtcga_class0_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m     \u001b[0mdf_eval\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_eval_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, **kwargs)\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstorage_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_nullable_dtypes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 500\u001b[0;31m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    501\u001b[0m     )\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, use_nullable_dtypes, storage_options, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m             result = self.api.parquet.read_table(\n\u001b[0;32m--> 240\u001b[0;31m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m             ).to_pandas(**to_pandas_kwargs)\n\u001b[1;32m    242\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmanager\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"array\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, metadata, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit)\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2986\u001b[0m         return dataset.read(columns=columns, use_threads=use_threads,\n\u001b[0;32m-> 2987\u001b[0;31m                             use_pandas_metadata=use_pandas_metadata)\n\u001b[0m\u001b[1;32m   2988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2989\u001b[0m     warnings.warn(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, columns, use_threads, use_pandas_metadata)\u001b[0m\n\u001b[1;32m   2614\u001b[0m         table = self._dataset.to_table(\n\u001b[1;32m   2615\u001b[0m             \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filter_expression\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2616\u001b[0;31m             \u001b[0muse_threads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_threads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2617\u001b[0m         )\n\u001b[1;32m   2618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\"\"\"BF 1 features CF generation\n",
    "Input : Same as DICE method\n",
    "Output : Same format as input, no original factual index as the end\"\"\"\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2000)\n",
    "        self.bn1 = nn.BatchNorm1d(2000)\n",
    "        self.dropout1 = nn.Dropout(0.02)\n",
    "        self.fc2 = nn.Linear(2000, 200)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.dropout2 = nn.Dropout(0.02)\n",
    "        self.fc3 = nn.Linear(200, 20)\n",
    "        self.bn3 = nn.BatchNorm1d(20)\n",
    "        self.dropout3 = nn.Dropout(0.02)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "def load_model(model_path, input_size, device):\n",
    "    \"\"\"Load the pretrained MLP model\"\"\"\n",
    "    model = MLP(input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def efficient_knn_distances(candidates, reference_data, device, chunk_size=32, k=10):\n",
    "    \"\"\"Memory-efficient k-NN computation with small chunks\"\"\"\n",
    "    n_candidates = candidates.shape[0]\n",
    "    n_reference = reference_data.shape[0]\n",
    "    k = min(k, n_reference)\n",
    "    \n",
    "    knn_distances = torch.zeros(n_candidates, device=device)\n",
    "    \n",
    "    for i in range(0, n_candidates, chunk_size):\n",
    "        end_i = min(i + chunk_size, n_candidates)\n",
    "        candidate_chunk = candidates[i:end_i]\n",
    "        \n",
    "        chunk_expanded = candidate_chunk.unsqueeze(1)\n",
    "        reference_expanded = reference_data.unsqueeze(0)\n",
    "        \n",
    "        distances = torch.sum(torch.abs(chunk_expanded - reference_expanded), dim=2)\n",
    "        topk_distances, _ = torch.topk(distances, k, dim=1, largest=False)\n",
    "        knn_distances[i:end_i] = topk_distances.mean(dim=1)\n",
    "        \n",
    "        del chunk_expanded, distances, topk_distances\n",
    "    \n",
    "    return knn_distances\n",
    "\n",
    "class AdvancedProgressTracker:\n",
    "    \"\"\"Advanced progress tracking with detailed time estimates and statistics\"\"\"\n",
    "    \n",
    "    def __init__(self, total_items, item_name, update_interval=5.0):\n",
    "        self.total_items = total_items\n",
    "        self.item_name = item_name\n",
    "        self.update_interval = update_interval\n",
    "        self.start_time = time.time()\n",
    "        self.last_update_time = self.start_time\n",
    "        self.completed_items = 0\n",
    "        self.speed_history = []\n",
    "        self.max_history = 20  # Keep last 20 speed measurements for smoothing\n",
    "        \n",
    "        # Display startup info\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"STARTING {item_name.upper()} COMPUTATION\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"Total {item_name}: {self.total_items:,}\")\n",
    "        print(f\"Started at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    def update(self, completed_items):\n",
    "        \"\"\"Update progress with detailed time estimates\"\"\"\n",
    "        self.completed_items = completed_items\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Update at specified intervals\n",
    "        if current_time - self.last_update_time >= self.update_interval:\n",
    "            elapsed_time = current_time - self.start_time\n",
    "            \n",
    "            if self.completed_items > 0:\n",
    "                # Calculate current speed\n",
    "                current_speed = self.completed_items / elapsed_time\n",
    "                self.speed_history.append(current_speed)\n",
    "                \n",
    "                # Keep only recent history for smoothing\n",
    "                if len(self.speed_history) > self.max_history:\n",
    "                    self.speed_history = self.speed_history[-self.max_history:]\n",
    "                \n",
    "                # Use smoothed speed for better estimates\n",
    "                avg_speed = sum(self.speed_history) / len(self.speed_history)\n",
    "                \n",
    "                # Calculate progress and time estimates\n",
    "                progress_pct = (self.completed_items / self.total_items) * 100\n",
    "                remaining_items = self.total_items - self.completed_items\n",
    "                \n",
    "                if avg_speed > 0:\n",
    "                    eta_seconds = remaining_items / avg_speed\n",
    "                    total_time_estimate = self.total_items / avg_speed\n",
    "                    \n",
    "                    # Format time strings\n",
    "                    eta_str = self._format_time(eta_seconds)\n",
    "                    total_str = self._format_time(total_time_estimate)\n",
    "                    elapsed_str = self._format_time(elapsed_time)\n",
    "                    \n",
    "                    # Create progress bar\n",
    "                    bar_length = 40\n",
    "                    filled_length = int(bar_length * progress_pct / 100)\n",
    "                    bar = '█' * filled_length + '░' * (bar_length - filled_length)\n",
    "                    \n",
    "                    # Calculate completion time\n",
    "                    completion_time = datetime.now() + timedelta(seconds=eta_seconds)\n",
    "                    \n",
    "                    # Dynamic update line\n",
    "                    progress_line = (\n",
    "                        f\"\\r{progress_pct:6.2f}% [{bar}] \"\n",
    "                        f\"{self.completed_items:,}/{self.total_items:,} {self.item_name} | \"\n",
    "                        f\"Elapsed: {elapsed_str} | ETA: {eta_str} | \"\n",
    "                        f\"Speed: {avg_speed:.0f}/sec | \"\n",
    "                        f\"Finish: {completion_time.strftime('%H:%M:%S')}\"\n",
    "                    )\n",
    "                    \n",
    "                    print(progress_line, end='', flush=True)\n",
    "                else:\n",
    "                    print(f\"\\rInitializing... {self.completed_items:,} {self.item_name} processed\", end='', flush=True)\n",
    "            \n",
    "            self.last_update_time = current_time\n",
    "    \n",
    "    def _format_time(self, seconds):\n",
    "        \"\"\"Format seconds into human-readable time string\"\"\"\n",
    "        if seconds < 60:\n",
    "            return f\"{seconds:.0f}s\"\n",
    "        elif seconds < 3600:\n",
    "            minutes = seconds / 60\n",
    "            return f\"{minutes:.1f}m\"\n",
    "        else:\n",
    "            hours = seconds / 3600\n",
    "            return f\"{hours:.1f}h\"\n",
    "    \n",
    "    def finish(self):\n",
    "        \"\"\"Print completion summary\"\"\"\n",
    "        print()  # New line after progress bar\n",
    "        total_time = time.time() - self.start_time\n",
    "        final_speed = self.total_items / total_time if total_time > 0 else 0\n",
    "        \n",
    "        print(\"=\" * 80)\n",
    "        print(f\"{self.item_name.upper()} COMPLETED!\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"Total time: {self._format_time(total_time)}\")\n",
    "        print(f\"Final speed: {final_speed:.0f} {self.item_name}/second\")\n",
    "        print(f\"Completed at: {datetime.now().strftime('%H:%M:%S')}\")\n",
    "        print(\"=\" * 80)\n",
    "\n",
    "def precompute_all_predictions(X_eval, all_feature_samples, model, device, \n",
    "                               num_points, batch_size=512, save_path=None):\n",
    "    \"\"\"\n",
    "    GAME CHANGER: Precompute ALL predictions for ALL feature variations\n",
    "    \n",
    "    This transforms the problem from:\n",
    "    - 863M individual model calls (super slow)\n",
    "    - To: ~86M predictions in large efficient batches (much faster)\n",
    "    \"\"\"\n",
    "    n_samples, n_features = X_eval.shape\n",
    "    total_predictions = n_samples * n_features * num_points\n",
    "    \n",
    "    print(f\"PRECOMPUTING ALL PREDICTIONS...\")\n",
    "    print(f\"Samples: {n_samples:,} | Features: {n_features:,} | Points: {num_points}\")\n",
    "    print(f\"Total predictions needed: {total_predictions:,}\")\n",
    "    print(f\"Memory required: ~{total_predictions * 4 / 1e9:.1f}GB\")\n",
    "    print()\n",
    "    \n",
    "    # Storage for all predictions: (n_samples, n_features, num_points)\n",
    "    all_predictions = torch.zeros(n_samples, n_features, num_points, device='cpu')\n",
    "    \n",
    "    # Initialize progress tracker for samples\n",
    "    sample_tracker = AdvancedProgressTracker(n_samples, \"samples\", update_interval=3.0)\n",
    "    \n",
    "    processed_predictions = 0\n",
    "    \n",
    "    # Process each sample\n",
    "    for sample_idx in range(n_samples):\n",
    "        sample = X_eval[sample_idx:sample_idx+1]  # (1, n_features)\n",
    "        \n",
    "        # Create ALL candidates for this sample at once\n",
    "        sample_candidates = []\n",
    "        \n",
    "        for feature_idx in range(n_features):\n",
    "            # Create candidates for this feature\n",
    "            feature_samples = all_feature_samples[feature_idx]\n",
    "            candidates_for_feature = sample.repeat(num_points, 1)  # (num_points, n_features)\n",
    "            candidates_for_feature[:, feature_idx] = feature_samples\n",
    "            sample_candidates.append(candidates_for_feature)\n",
    "        \n",
    "        # Stack all candidates for this sample: (n_features * num_points, n_features)\n",
    "        all_sample_candidates = torch.cat(sample_candidates, dim=0)\n",
    "        \n",
    "        # Batch predict ALL candidates for this sample\n",
    "        sample_predictions = torch.zeros(all_sample_candidates.shape[0], device=device)\n",
    "        \n",
    "        for i in range(0, all_sample_candidates.shape[0], batch_size):\n",
    "            end_i = min(i + batch_size, all_sample_candidates.shape[0])\n",
    "            batch_candidates = all_sample_candidates[i:end_i].to(device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                batch_preds = model(batch_candidates).squeeze()\n",
    "                if batch_preds.dim() == 0:\n",
    "                    batch_preds = batch_preds.unsqueeze(0)\n",
    "                sample_predictions[i:end_i] = batch_preds\n",
    "            \n",
    "            del batch_candidates\n",
    "        \n",
    "        # Reshape predictions back to (n_features, num_points)\n",
    "        sample_predictions = sample_predictions.view(n_features, num_points)\n",
    "        all_predictions[sample_idx] = sample_predictions.cpu()\n",
    "        \n",
    "        processed_predictions += n_features * num_points\n",
    "        \n",
    "        # Update progress tracker\n",
    "        sample_tracker.update(sample_idx + 1)\n",
    "        \n",
    "        # Clean up\n",
    "        del sample_candidates, all_sample_candidates, sample_predictions\n",
    "        \n",
    "        # Periodic GPU memory cleanup\n",
    "        if (sample_idx + 1) % 50 == 0:\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Finish progress tracking\n",
    "    sample_tracker.finish()\n",
    "    \n",
    "    # Calculate final statistics\n",
    "    total_time = time.time() - sample_tracker.start_time\n",
    "    prediction_rate = total_predictions / total_time if total_time > 0 else 0\n",
    "    \n",
    "    print(f\"PRECOMPUTATION STATISTICS:\")\n",
    "    print(f\"- Total predictions: {total_predictions:,}\")\n",
    "    print(f\"- Prediction rate: {prediction_rate:.0f} predictions/second\")\n",
    "    print(f\"- Memory efficiency: {total_predictions * 4 / 1e6:.0f} MB stored\")\n",
    "    \n",
    "    # Optionally save precomputed predictions\n",
    "    if save_path:\n",
    "        print(f\"\\nSaving precomputed predictions to {save_path}...\")\n",
    "        save_start = time.time()\n",
    "        torch.save(all_predictions, save_path)\n",
    "        save_time = time.time() - save_start\n",
    "        file_size = all_predictions.numel() * 4 / 1e9\n",
    "        print(f\"Saved {file_size:.1f}GB in {save_time:.1f}s ({file_size/save_time:.1f}GB/s)\")\n",
    "    \n",
    "    return all_predictions\n",
    "\n",
    "def compute_scores_with_precomputed_predictions(\n",
    "    tcga_class0_path,\n",
    "    data_eval_path,\n",
    "    model_path,\n",
    "    num_points=50,\n",
    "    range_percent=10,\n",
    "    diff_weight=30,\n",
    "    nn_weight=10,\n",
    "    l1_weight=10,\n",
    "    gpu_id=0,\n",
    "    batch_size=16,\n",
    "    prediction_batch_size=512,     # Batch size for precomputation\n",
    "    knn_chunk_size=16,\n",
    "    precompute_save_path='precomputed_predictions.pt',\n",
    "    output_cf_path='best_counterfactuals.parquet',\n",
    "    output_importance_path='feature_importance.parquet'\n",
    "):\n",
    "    \"\"\"\n",
    "    Ultra-fast version using precomputed predictions\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # GPU setup\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "    \n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Constants\n",
    "    class0_10nn_mean = 5950.676427\n",
    "    class0_10nn_std = np.sqrt(3400381.504375)\n",
    "    l1_mean = 6471.3016\n",
    "    l1_std = 1596.4753\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading data...\")\n",
    "    df_class0 = pd.read_parquet(tcga_class0_path)\n",
    "    df_eval = pd.read_parquet(data_eval_path)\n",
    "    \n",
    "    X_class0 = df_class0.iloc[:, 1:].values\n",
    "    X_eval = df_eval.iloc[:, 1:].values\n",
    "    \n",
    "    num_features = X_eval.shape[1]\n",
    "    n_samples = X_eval.shape[0]\n",
    "    print(f\"Data: {n_samples} samples, {num_features} features\")\n",
    "    \n",
    "    # Handle dimension mismatch\n",
    "    if X_class0.shape[1] != num_features:\n",
    "        if X_class0.shape[1] < num_features:\n",
    "            padding = np.zeros((X_class0.shape[0], num_features - X_class0.shape[1]))\n",
    "            X_class0 = np.hstack((X_class0, padding))\n",
    "        else:\n",
    "            X_class0 = X_class0[:, :num_features]\n",
    "    \n",
    "    # Move class0 to GPU\n",
    "    X_class0_tensor = torch.tensor(X_class0, dtype=torch.float32).to(device)\n",
    "    X_eval_tensor = torch.tensor(X_eval, dtype=torch.float32)\n",
    "    \n",
    "    # Precompute feature ranges\n",
    "    print(\"Computing feature ranges...\")\n",
    "    feature_min = X_eval.min(axis=0)\n",
    "    feature_max = X_eval.max(axis=0)\n",
    "    feature_range = (feature_max - feature_min) * (range_percent / 100)\n",
    "    \n",
    "    # Precompute sampling points\n",
    "    all_feature_samples = []\n",
    "    for i in range(num_features):\n",
    "        min_val = feature_min[i] - feature_range[i]\n",
    "        max_val = feature_max[i] + feature_range[i]\n",
    "        samples = torch.linspace(min_val, max_val, num_points, dtype=torch.float32)\n",
    "        all_feature_samples.append(samples)\n",
    "    \n",
    "    # Load model\n",
    "    model = load_model(model_path, num_features, device)\n",
    "    \n",
    "    # Get initial predictions\n",
    "    print(\"Computing initial predictions...\")\n",
    "    initial_predictions = torch.zeros(n_samples, device='cpu')\n",
    "    for i in range(0, n_samples, prediction_batch_size):\n",
    "        end_i = min(i + prediction_batch_size, n_samples)\n",
    "        with torch.no_grad():\n",
    "            batch_pred = model(X_eval_tensor[i:end_i].to(device)).squeeze().cpu()\n",
    "            if batch_pred.dim() == 0:\n",
    "                batch_pred = batch_pred.unsqueeze(0)\n",
    "            initial_predictions[i:end_i] = batch_pred\n",
    "    \n",
    "    # GAME CHANGER: Precompute ALL predictions\n",
    "    try:\n",
    "        print(\"Loading precomputed predictions...\")\n",
    "        all_predictions = torch.load(precompute_save_path)\n",
    "        print(f\"Loaded precomputed predictions: {all_predictions.shape}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"Precomputed predictions not found. Computing them now...\")\n",
    "        all_predictions = precompute_all_predictions(\n",
    "            X_eval_tensor, all_feature_samples, model, device, \n",
    "            num_points, prediction_batch_size, precompute_save_path\n",
    "        )\n",
    "    \n",
    "    # Now the scoring phase is SUPER FAST - just lookups!\n",
    "    print(\"\\n🚀 Starting ULTRA-FAST scoring with precomputed predictions...\")\n",
    "    \n",
    "    best_counterfactuals = []\n",
    "    feature_importance_data = []\n",
    "    \n",
    "    # Create dataloader\n",
    "    eval_dataset = TensorDataset(X_eval_tensor)\n",
    "    eval_dataloader = DataLoader(eval_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Initialize scoring progress tracker\n",
    "    total_operations = n_samples * num_features\n",
    "    scoring_tracker = AdvancedProgressTracker(total_operations, \"feature evaluations\", update_interval=2.0)\n",
    "    \n",
    "    completed_operations = 0\n",
    "    \n",
    "    # Process samples\n",
    "    for batch_idx, (X_batch,) in enumerate(eval_dataloader):\n",
    "        batch_size_actual = X_batch.size(0)\n",
    "        batch_start_idx = batch_idx * batch_size\n",
    "        \n",
    "        for i in range(batch_size_actual):\n",
    "            sample_idx = batch_start_idx + i\n",
    "            if sample_idx >= n_samples:\n",
    "                break\n",
    "                \n",
    "            eval_sample = X_batch[i:i+1]\n",
    "            initial_pred = initial_predictions[sample_idx].item()\n",
    "            \n",
    "            best_score = float('-inf')\n",
    "            best_cf = eval_sample.clone()\n",
    "            best_cf_pred = initial_pred\n",
    "            sample_importance = []\n",
    "            \n",
    "            # Process each feature (now SUPER FAST - no model calls!)\n",
    "            for feature_idx in range(num_features):\n",
    "                # Get precomputed predictions for this feature (INSTANT!)\n",
    "                feature_predictions = all_predictions[sample_idx, feature_idx]  # (num_points,)\n",
    "                \n",
    "                # Create candidates for k-NN and L1 computation\n",
    "                feature_samples = all_feature_samples[feature_idx]\n",
    "                candidates = eval_sample.repeat(num_points, 1)\n",
    "                candidates[:, feature_idx] = feature_samples\n",
    "                \n",
    "                # Move to GPU for k-NN computation\n",
    "                candidates_gpu = candidates.to(device)\n",
    "                \n",
    "                # Efficient k-NN computation\n",
    "                nn_distances = efficient_knn_distances(\n",
    "                    candidates_gpu, X_class0_tensor, device, \n",
    "                    chunk_size=knn_chunk_size, k=10\n",
    "                )\n",
    "                \n",
    "                # L1 distances to original sample\n",
    "                l1_distances = torch.sum(torch.abs(candidates - eval_sample), dim=1)\n",
    "                \n",
    "                # Compute scores using precomputed predictions\n",
    "                pred_diffs = initial_pred - feature_predictions\n",
    "                nn_scores = torch.abs(nn_distances.cpu() - class0_10nn_mean) / class0_10nn_std\n",
    "                l1_scores = torch.abs(l1_distances - l1_mean) / l1_std\n",
    "                \n",
    "                total_scores = (pred_diffs * diff_weight) - (nn_scores * nn_weight) - (l1_scores * l1_weight)\n",
    "                \n",
    "                # Find best candidate for this feature\n",
    "                best_idx = torch.argmax(total_scores)\n",
    "                feature_best_score = total_scores[best_idx]\n",
    "                feature_best_pred = feature_predictions[best_idx]\n",
    "                feature_best_candidate = candidates[best_idx]\n",
    "                \n",
    "                # Store feature importance\n",
    "                sample_importance.append((initial_pred - feature_best_pred).item())\n",
    "                \n",
    "                # Update global best\n",
    "                if feature_best_score > best_score:\n",
    "                    best_score = feature_best_score\n",
    "                    best_cf = feature_best_candidate.unsqueeze(0)\n",
    "                    best_cf_pred = feature_best_pred.item()\n",
    "                \n",
    "                # Minimal cleanup\n",
    "                del candidates_gpu, nn_distances\n",
    "                \n",
    "                # Update progress tracker\n",
    "                completed_operations += 1\n",
    "                scoring_tracker.update(completed_operations)\n",
    "            \n",
    "            # Store results\n",
    "            cf_row = [best_cf_pred] + best_cf[0].numpy().tolist()\n",
    "            best_counterfactuals.append(cf_row)\n",
    "            feature_importance_data.append(sample_importance)\n",
    "    \n",
    "    # Finish scoring progress\n",
    "    scoring_tracker.finish()\n",
    "    \n",
    "    # Create and save results\n",
    "    print(\"Creating output DataFrames...\")\n",
    "    cf_columns = ['classification'] + [f'feature_{i}' for i in range(num_features)]\n",
    "    importance_columns = [f'feature_{i}_diff' for i in range(num_features)]\n",
    "    \n",
    "    best_cf_df = pd.DataFrame(best_counterfactuals, columns=cf_columns)\n",
    "    importance_df = pd.DataFrame(feature_importance_data, columns=importance_columns)\n",
    "    \n",
    "    # Save results\n",
    "    best_cf_df.to_parquet(output_cf_path, engine='pyarrow', compression='snappy', index=False)\n",
    "    importance_df.to_parquet(output_importance_path, engine='pyarrow', compression='snappy', index=False)\n",
    "    \n",
    "    print(f\"Saved: {output_cf_path} and {output_importance_path}\")\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    precompute_time = scoring_tracker.start_time - start_time\n",
    "    scoring_time = time.time() - scoring_tracker.start_time\n",
    "    \n",
    "    print(f\"\\n🏁 FINAL TIMING SUMMARY:\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"TOTAL EXECUTION TIME: {total_time/60:.1f} minutes ({total_time/3600:.1f} hours)\")\n",
    "    print(f\"├─ Precomputation: {precompute_time/60:.1f} minutes ({precompute_time/total_time*100:.1f}%)\")\n",
    "    print(f\"├─ Scoring: {scoring_time/60:.1f} minutes ({scoring_time/total_time*100:.1f}%)\")\n",
    "    print(f\"└─ Other: {(total_time-precompute_time-scoring_time)/60:.1f} minutes\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Final cleanup\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    return best_cf_df, importance_df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "bf1_cfs, feature_importances = compute_scores_with_precomputed_predictions(\n",
    "    tcga_class0_path='tcga_class0.parquet',\n",
    "    data_eval_path='data_eval_norm.parquet',\n",
    "    model_path='mlp_model.pth',\n",
    "    num_points=25,\n",
    "    range_percent=0,\n",
    "    diff_weight=30,\n",
    "    nn_weight=10,\n",
    "    l1_weight=10,\n",
    "    batch_size=16,\n",
    "    prediction_batch_size=2048,\n",
    "    knn_chunk_size=64,\n",
    "    gpu_id=0\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"BF 2 features CF generation\n",
    "Input : Same as DICE method\n",
    "Output : Same format as input, no original factual index as the end\"\"\"\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import gc\n",
    "import json\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 2000)\n",
    "        self.bn1 = nn.BatchNorm1d(2000)\n",
    "        self.dropout1 = nn.Dropout(0.02)\n",
    "        self.fc2 = nn.Linear(2000, 200)\n",
    "        self.bn2 = nn.BatchNorm1d(200)\n",
    "        self.dropout2 = nn.Dropout(0.02)\n",
    "        self.fc3 = nn.Linear(200, 20)\n",
    "        self.bn3 = nn.BatchNorm1d(20)\n",
    "        self.dropout3 = nn.Dropout(0.02)\n",
    "        self.fc4 = nn.Linear(20, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.leaky_relu = nn.LeakyReLU(0.01)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.leaky_relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.leaky_relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.leaky_relu(self.bn3(self.fc3(x)))\n",
    "        x = self.dropout3(x)\n",
    "        x = self.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "class PyTorchOCSVM(nn.Module):\n",
    "    def __init__(self, input_dim, nu=0.1, device='cuda:0'):\n",
    "        super(PyTorchOCSVM, self).__init__()\n",
    "        self.nu = nu\n",
    "        self.device = device\n",
    "        self.input_dim = input_dim\n",
    "        \n",
    "        self.feature_net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(512, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        ).to(device)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.feature_net(x)\n",
    "\n",
    "class OptimizedGPUOCSVM:\n",
    "    \"\"\"GPU-native OSVM with adaptive batch sizing and conservative fp16 usage\"\"\"\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.scaler_mean = None\n",
    "        self.scaler_scale = None\n",
    "        self.center = None\n",
    "        self.rho = 0.0\n",
    "        self.device = None\n",
    "        self.batch_size = 1024\n",
    "        \n",
    "    def load_model(self, filepath):\n",
    "        checkpoint = torch.load(filepath, map_location='cuda:0')\n",
    "        \n",
    "        self.nu = checkpoint['nu']\n",
    "        self.rho = checkpoint['rho']\n",
    "        self.center = checkpoint['center'].to('cuda:0') if checkpoint['center'] is not None else None\n",
    "        self.device = 'cuda:0'\n",
    "        \n",
    "        # Keep scaler parameters in float32 for numerical stability\n",
    "        scaler = checkpoint['scaler']\n",
    "        self.scaler_mean = torch.tensor(scaler.mean_, dtype=torch.float32, device='cuda:0')\n",
    "        self.scaler_scale = torch.tensor(scaler.scale_, dtype=torch.float32, device='cuda:0')\n",
    "        \n",
    "        input_dim = checkpoint['input_dim']\n",
    "        self.model = PyTorchOCSVM(input_dim, self.nu, self.device)\n",
    "        self.model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        self.model.to(self.device)\n",
    "        self.model.eval()\n",
    "        \n",
    "        # JIT compile for speed\n",
    "        dummy_input = torch.randn(1, input_dim, device=self.device)\n",
    "        self.model = torch.jit.trace(self.model, dummy_input)\n",
    "        \n",
    "        # Find optimal batch size for this hardware\n",
    "        self.batch_size = self._find_optimal_batch_size(input_dim)\n",
    "        \n",
    "        print(f\"OSVM model loaded, JIT compiled, optimal batch size: {self.batch_size}\")\n",
    "    \n",
    "    def _find_optimal_batch_size(self, n_features):\n",
    "        \"\"\"Find optimal batch size based on available VRAM\"\"\"\n",
    "        for batch_size in [2048, 1536, 1024, 768, 512, 256]:\n",
    "            try:\n",
    "                dummy = torch.randn(batch_size, n_features, device=self.device)\n",
    "                with torch.no_grad():\n",
    "                    batch_scaled = (dummy - self.scaler_mean) / self.scaler_scale\n",
    "                    with autocast():\n",
    "                        _ = self.model(batch_scaled)\n",
    "                del dummy, batch_scaled\n",
    "                torch.cuda.empty_cache()\n",
    "                return batch_size\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                continue\n",
    "        \n",
    "        return 128  # Conservative fallback\n",
    "    \n",
    "    def decision_function(self, X_tensor):\n",
    "        \"\"\"GPU-native decision function with mixed precision where safe\"\"\"\n",
    "        decisions = torch.empty(len(X_tensor), device=self.device, dtype=torch.float32)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(0, len(X_tensor), self.batch_size):\n",
    "                end_idx = min(i + self.batch_size, len(X_tensor))\n",
    "                batch = X_tensor[i:end_idx]\n",
    "                \n",
    "                # Keep scaling in float32 for numerical stability\n",
    "                batch_scaled = (batch - self.scaler_mean) / self.scaler_scale\n",
    "                \n",
    "                # Use mixed precision for forward pass only\n",
    "                with autocast():\n",
    "                    outputs = self.model(batch_scaled)\n",
    "                \n",
    "                # Distance calculation in float32 for precision\n",
    "                distances = torch.sum((outputs.float() - self.center) ** 2, dim=1)\n",
    "                decisions[i:end_idx] = -distances\n",
    "        \n",
    "        return decisions\n",
    "\n",
    "def load_model(model_path, input_size, device):\n",
    "    \"\"\"Load and JIT compile MLP model\"\"\"\n",
    "    model = MLP(input_size).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval()\n",
    "    \n",
    "    # JIT compile for speed\n",
    "    dummy_input = torch.randn(1, input_size, device=device)\n",
    "    model = torch.jit.trace(model, dummy_input)\n",
    "    print(\"MLP model loaded and JIT compiled\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def compute_feature_ranges(X_eval, range_percent):\n",
    "    \"\"\"Compute feature ranges using GPU quantiles\"\"\"\n",
    "    if range_percent < 0:\n",
    "        lower_percentile = abs(range_percent)\n",
    "        upper_percentile = 100 - abs(range_percent)\n",
    "        min_vals = torch.quantile(X_eval, lower_percentile / 100.0, dim=0)\n",
    "        max_vals = torch.quantile(X_eval, upper_percentile / 100.0, dim=0)\n",
    "        print(f\"Using percentile range: {lower_percentile}% to {upper_percentile}%\")\n",
    "    else:\n",
    "        feature_min = X_eval.min(dim=0)[0]\n",
    "        feature_max = X_eval.max(dim=0)[0]\n",
    "        feature_range = (feature_max - feature_min) * (range_percent / 100)\n",
    "        min_vals = feature_min - feature_range\n",
    "        max_vals = feature_max + feature_range\n",
    "        print(f\"Using expanded range: ±{range_percent}% beyond observed min/max\")\n",
    "    \n",
    "    return min_vals, max_vals\n",
    "\n",
    "def warmup_models(mlp_model, osvm_model, n_features, device):\n",
    "    \"\"\"Pre-warm models to trigger CUDA kernel compilation\"\"\"\n",
    "    print(\"Warming up models...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        warmup_data = torch.randn(64, n_features, device=device)\n",
    "        \n",
    "        with autocast():\n",
    "            _ = mlp_model(warmup_data)\n",
    "        \n",
    "        _ = osvm_model.decision_function(warmup_data)\n",
    "        \n",
    "        del warmup_data\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Model warmup completed\")\n",
    "\n",
    "def process_candidates_chunked(sample, first_feat_idx, second_feat_global_idx, \n",
    "                              first_samples, second_samples, mlp_model, osvm_model,\n",
    "                              prediction_weight, distance_weight, osvm_weight,\n",
    "                              L1_NORM_MEAN, L1_NORM_STD, use_mixed_precision, \n",
    "                              chunk_size=1024):\n",
    "    \"\"\"Process candidates in memory-efficient chunks with CORRECTED SCORING\"\"\"\n",
    "    \n",
    "    grid_first, grid_second = torch.meshgrid(first_samples, second_samples, indexing='ij')\n",
    "    total_candidates = grid_first.numel()\n",
    "    \n",
    "    best_score = float('-inf')  # Initialize to negative infinity for ARGMAX\n",
    "    best_pred = 0.0\n",
    "    best_candidate = sample.clone()\n",
    "    \n",
    "    # Process in chunks to limit memory usage\n",
    "    for chunk_start in range(0, total_candidates, chunk_size):\n",
    "        chunk_end = min(chunk_start + chunk_size, total_candidates)\n",
    "        chunk_size_actual = chunk_end - chunk_start\n",
    "        \n",
    "        # Create chunk candidates\n",
    "        candidates_chunk = sample.unsqueeze(0).expand(chunk_size_actual, -1).contiguous()\n",
    "        candidates_chunk[:, first_feat_idx] = grid_first.flatten()[chunk_start:chunk_end]\n",
    "        candidates_chunk[:, second_feat_global_idx] = grid_second.flatten()[chunk_start:chunk_end]\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Process chunk\n",
    "            if use_mixed_precision:\n",
    "                with autocast():\n",
    "                    chunk_preds = mlp_model(candidates_chunk).squeeze()\n",
    "            else:\n",
    "                chunk_preds = mlp_model(candidates_chunk).squeeze()\n",
    "            \n",
    "            if chunk_preds.dim() == 0:\n",
    "                chunk_preds = chunk_preds.unsqueeze(0)\n",
    "            \n",
    "            chunk_osvm = osvm_model.decision_function(candidates_chunk)\n",
    "            chunk_l1 = torch.sum(torch.abs(candidates_chunk - sample), dim=1)\n",
    "            \n",
    "            # CORRECTED SCORING FORMULA:\n",
    "            # score = -mlp_prediction × prediction_weight - abs(distance-mean)/std + (1-osvm_score) × osvm_weight\n",
    "            prediction_component = -chunk_preds * prediction_weight\n",
    "            distance_component = -(torch.abs(chunk_l1 - L1_NORM_MEAN) / L1_NORM_STD) * distance_weight\n",
    "            osvm_component = (1 - chunk_osvm) * osvm_weight\n",
    "            \n",
    "            chunk_scores = prediction_component + distance_component + osvm_component\n",
    "            \n",
    "            # Update best from this chunk (ARGMAX - find maximum score)\n",
    "            chunk_best_idx = torch.argmax(chunk_scores)\n",
    "            chunk_best_score = chunk_scores[chunk_best_idx].item()\n",
    "            \n",
    "            if chunk_best_score > best_score:  # GREATER THAN for maximum\n",
    "                best_score = chunk_best_score\n",
    "                best_pred = chunk_preds[chunk_best_idx].item()\n",
    "                best_candidate = candidates_chunk[chunk_best_idx].clone()\n",
    "        \n",
    "        # Cleanup chunk\n",
    "        del candidates_chunk, chunk_preds, chunk_osvm, chunk_l1, chunk_scores\n",
    "    \n",
    "    return best_score, best_pred, best_candidate\n",
    "\n",
    "class ProgressTracker:\n",
    "    \"\"\"Comprehensive progress tracking with detailed statistics\"\"\"\n",
    "    \n",
    "    def __init__(self, total_samples, total_features, n_second_features, num_points_first, num_points_second):\n",
    "        self.total_samples = total_samples\n",
    "        self.total_features = total_features\n",
    "        self.n_second_features = n_second_features\n",
    "        self.num_points_first = num_points_first\n",
    "        self.num_points_second = num_points_second\n",
    "        \n",
    "        # Calculate total work\n",
    "        self.grid_size = num_points_first * num_points_second\n",
    "        self.total_evaluations = total_samples * total_features * n_second_features * self.grid_size\n",
    "        \n",
    "        # Timing\n",
    "        self.start_time = time.time()\n",
    "        self.last_update_time = self.start_time\n",
    "        self.update_interval = 20  # Update every 20 seconds\n",
    "        \n",
    "        # Progress counters\n",
    "        self.completed_samples = 0\n",
    "        self.completed_evaluations = 0\n",
    "        \n",
    "        # Speed tracking\n",
    "        self.speed_history = []\n",
    "        self.max_speed_history = 10\n",
    "        \n",
    "        self.print_startup_summary()\n",
    "    \n",
    "    def print_startup_summary(self):\n",
    "        \"\"\"Print comprehensive startup information\"\"\"\n",
    "        print(f\"\\n{'='*85}\")\n",
    "        print(f\"COUNTERFACTUAL GENERATION STARTED\")\n",
    "        print(f\"{'='*85}\")\n",
    "        print(f\"Dataset Configuration:\")\n",
    "        print(f\"   • Total samples: {self.total_samples:,}\")\n",
    "        print(f\"   • Primary features: {self.total_features:,}\")\n",
    "        print(f\"   • Secondary features: {self.n_second_features:,}\")\n",
    "        print(f\"Grid Configuration:\")\n",
    "        print(f\"   • Primary grid points: {self.num_points_first}\")\n",
    "        print(f\"   • Secondary grid points: {self.num_points_second}\")\n",
    "        print(f\"   • Grid size per pair: {self.grid_size:,}\")\n",
    "        print(f\"Computation Scale:\")\n",
    "        print(f\"   • Total evaluations: {self.total_evaluations:,}\")\n",
    "        print(f\"   • Evaluations per sample: {self.total_evaluations // self.total_samples:,}\")\n",
    "        print(f\"Started at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"{'='*85}\\n\")\n",
    "    \n",
    "    def update(self, sample_idx, feature_idx=None, force_update=False):\n",
    "        \"\"\"Update progress with detailed tracking\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Update counters\n",
    "        self.completed_samples = sample_idx + 1\n",
    "        if feature_idx is not None:\n",
    "            # Calculate completed evaluations based on current progress\n",
    "            base_evaluations = sample_idx * self.total_features * self.n_second_features * self.grid_size\n",
    "            current_feature_evaluations = feature_idx * self.n_second_features * self.grid_size\n",
    "            self.completed_evaluations = base_evaluations + current_feature_evaluations\n",
    "        else:\n",
    "            # If no feature index, assume sample is complete\n",
    "            self.completed_evaluations = self.completed_samples * self.total_features * self.n_second_features * self.grid_size\n",
    "        \n",
    "        # Check if we should update display\n",
    "        should_update = (\n",
    "            force_update or \n",
    "            (current_time - self.last_update_time >= self.update_interval) or\n",
    "            sample_idx == 0 or\n",
    "            sample_idx == self.total_samples - 1\n",
    "        )\n",
    "        \n",
    "        if should_update:\n",
    "            self.last_update_time = current_time\n",
    "            self._display_progress(current_time)\n",
    "    \n",
    "    def _display_progress(self, current_time):\n",
    "        \"\"\"Display comprehensive progress information\"\"\"\n",
    "        elapsed_time = current_time - self.start_time\n",
    "        \n",
    "        # Calculate progress percentage\n",
    "        progress_pct = (self.completed_evaluations / self.total_evaluations) * 100 if self.total_evaluations > 0 else 0\n",
    "        sample_progress_pct = (self.completed_samples / self.total_samples) * 100\n",
    "        \n",
    "        # Calculate speed metrics\n",
    "        if elapsed_time > 0:\n",
    "            current_speed = self.completed_evaluations / elapsed_time\n",
    "            self.speed_history.append(current_speed)\n",
    "            if len(self.speed_history) > self.max_speed_history:\n",
    "                self.speed_history.pop(0)\n",
    "            avg_speed = sum(self.speed_history) / len(self.speed_history)\n",
    "        else:\n",
    "            avg_speed = 0\n",
    "        \n",
    "        # Calculate ETA\n",
    "        if avg_speed > 0:\n",
    "            remaining_evaluations = self.total_evaluations - self.completed_evaluations\n",
    "            eta_seconds = remaining_evaluations / avg_speed\n",
    "            finish_time = datetime.now() + timedelta(seconds=eta_seconds)\n",
    "        else:\n",
    "            eta_seconds = 0\n",
    "            finish_time = datetime.now()\n",
    "        \n",
    "        # Format time strings\n",
    "        elapsed_str = self._format_duration(elapsed_time)\n",
    "        eta_str = self._format_duration(eta_seconds)\n",
    "        \n",
    "        # Create progress bar\n",
    "        bar_width = 40\n",
    "        filled_width = int(bar_width * progress_pct / 100)\n",
    "        bar = '█' * filled_width + '░' * (bar_width - filled_width)\n",
    "        \n",
    "        # Display progress\n",
    "        print(f\"\\r{progress_pct:6.2f}% [{bar}] \"\n",
    "              f\"Sample: {self.completed_samples:4d}/{self.total_samples} ({sample_progress_pct:5.1f}%) | \"\n",
    "              f\"Elapsed: {elapsed_str} | ETA: {eta_str} | \"\n",
    "              f\"Speed: {avg_speed:8,.0f} eval/s | \"\n",
    "              f\"Finish: {finish_time.strftime('%m-%d %H:%M')}\", end='', flush=True)\n",
    "    \n",
    "    def _format_duration(self, seconds):\n",
    "        \"\"\"Format duration in human readable format\"\"\"\n",
    "        if seconds < 60:\n",
    "            return f\"{seconds:.0f}s\"\n",
    "        elif seconds < 3600:\n",
    "            return f\"{seconds/60:.1f}m\"\n",
    "        elif seconds < 86400:\n",
    "            return f\"{seconds/3600:.1f}h\"\n",
    "        else:\n",
    "            return f\"{seconds/86400:.1f}d\"\n",
    "    \n",
    "    def finish(self):\n",
    "        \"\"\"Display final completion summary\"\"\"\n",
    "        total_time = time.time() - self.start_time\n",
    "        avg_speed = self.completed_evaluations / total_time if total_time > 0 else 0\n",
    "        \n",
    "        print(f\"\\n\\n{'='*85}\")\n",
    "        print(f\"COUNTERFACTUAL GENERATION COMPLETED!\")\n",
    "        print(f\"{'='*85}\")\n",
    "        print(f\"Final Statistics:\")\n",
    "        print(f\"   • Total runtime: {self._format_duration(total_time)}\")\n",
    "        print(f\"   • Evaluations completed: {self.completed_evaluations:,}\")\n",
    "        print(f\"   • Samples processed: {self.completed_samples:,}\")\n",
    "        print(f\"   • Average speed: {avg_speed:,.0f} evaluations/second\")\n",
    "        print(f\"Finished at: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "        print(f\"{'='*85}\\n\")\n",
    "\n",
    "def generate_2feature_counterfactuals_osvm_optimized(\n",
    "    data_eval_path,\n",
    "    model_path,\n",
    "    osvm_model_path,\n",
    "    second_feature_json_path,\n",
    "    num_points_first=25,\n",
    "    num_points_second=15,\n",
    "    range_percent=-10,\n",
    "    classification_weight=10.0,\n",
    "    distance_weight=10.0,\n",
    "    osvm_weight=10.0,\n",
    "    gpu_id=0,\n",
    "    use_mixed_precision=True,\n",
    "    prediction_batch_size=2048,\n",
    "    use_chunked_processing=True,\n",
    "    chunk_size=1024,\n",
    "    output_cf_path='best_counterfactuals_2feat_osvm.parquet',\n",
    "    output_importance_path='feature_importance_2feat_osvm.parquet'\n",
    "):\n",
    "    \"\"\"\n",
    "    CORRECTED VERSION with comprehensive progress tracking\n",
    "    Optimized counterfactual generation with conservative fp16 usage:\n",
    "    - fp16 for large storage tensors (candidates, samples)\n",
    "    - Mixed precision for model inference\n",
    "    - fp32 for all distance calculations and final scoring\n",
    "    \"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # GPU setup\n",
    "    device = torch.device(f\"cuda:{gpu_id}\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        torch.cuda.synchronize()\n",
    "        torch.backends.cudnn.benchmark = True\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        print(f\"Using device: {device} with RTX 2080 Ti optimizations\")\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading evaluation data...\")\n",
    "    df_eval = pd.read_parquet(data_eval_path)\n",
    "    X_eval = df_eval.iloc[:, 1:].values\n",
    "    X_eval_tensor = torch.tensor(X_eval, dtype=torch.float32, device=device)\n",
    "    \n",
    "    n_samples, n_features = X_eval_tensor.shape\n",
    "    print(f\"Data: {n_samples} samples, {n_features} features\")\n",
    "    \n",
    "    # Load models\n",
    "    print(\"Loading models...\")\n",
    "    mlp_model = load_model(model_path, n_features, device)\n",
    "    osvm_model = OptimizedGPUOCSVM()\n",
    "    osvm_model.load_model(osvm_model_path)\n",
    "    \n",
    "    warmup_models(mlp_model, osvm_model, n_features, device)\n",
    "    \n",
    "    # Load second feature indices\n",
    "    print(f\"Loading second feature indices...\")\n",
    "    try:\n",
    "        with open(second_feature_json_path, 'r') as f:\n",
    "            second_feature_indices = json.load(f)\n",
    "        \n",
    "        valid_indices = [idx for idx in second_feature_indices \n",
    "                        if isinstance(idx, int) and 0 <= idx < n_features]\n",
    "        second_feature_indices = torch.tensor(valid_indices, device=device, dtype=torch.long)\n",
    "        \n",
    "        if len(second_feature_indices) == 0:\n",
    "            print(\"Error: No valid second feature indices found\")\n",
    "            return None, None\n",
    "            \n",
    "    except (FileNotFoundError, json.JSONDecodeError) as e:\n",
    "        print(f\"Error loading JSON: {e}\")\n",
    "        return None, None\n",
    "    \n",
    "    n_second_features = len(second_feature_indices)\n",
    "    print(f\"Loaded {n_second_features} second feature indices\")\n",
    "    \n",
    "    # Compute feature ranges\n",
    "    print(\"Computing feature ranges...\")\n",
    "    min_vals, max_vals = compute_feature_ranges(X_eval_tensor, range_percent)\n",
    "    \n",
    "    # Pre-allocate sampling tensors\n",
    "    print(\"Pre-allocating sampling points...\")\n",
    "    first_feature_samples = torch.empty(n_features, num_points_first, device=device, dtype=torch.float16)\n",
    "    for i in range(n_features):\n",
    "        first_feature_samples[i] = torch.linspace(min_vals[i], max_vals[i], num_points_first, device=device, dtype=torch.float16)\n",
    "    \n",
    "    second_feature_samples = torch.empty(n_second_features, num_points_second, device=device, dtype=torch.float16)\n",
    "    for i, idx in enumerate(second_feature_indices):\n",
    "        second_feature_samples[i] = torch.linspace(min_vals[idx], max_vals[idx], num_points_second, device=device, dtype=torch.float16)\n",
    "    \n",
    "    # Get initial predictions\n",
    "    print(\"Computing initial predictions...\")\n",
    "    initial_predictions = torch.zeros(n_samples, device=device, dtype=torch.float32)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_samples, prediction_batch_size):\n",
    "            end_i = min(i + prediction_batch_size, n_samples)\n",
    "            if use_mixed_precision:\n",
    "                with autocast():\n",
    "                    batch_pred = mlp_model(X_eval_tensor[i:end_i]).squeeze()\n",
    "            else:\n",
    "                batch_pred = mlp_model(X_eval_tensor[i:end_i]).squeeze()\n",
    "            \n",
    "            if batch_pred.dim() == 0:\n",
    "                batch_pred = batch_pred.unsqueeze(0)\n",
    "            initial_predictions[i:end_i] = batch_pred\n",
    "    \n",
    "    # Initialize progress tracker\n",
    "    progress_tracker = ProgressTracker(\n",
    "        total_samples=n_samples,\n",
    "        total_features=n_features,\n",
    "        n_second_features=n_second_features,\n",
    "        num_points_first=num_points_first,\n",
    "        num_points_second=num_points_second\n",
    "    )\n",
    "    \n",
    "    # MAIN PROCESSING LOOP\n",
    "    print(\"Starting counterfactual generation...\\n\")\n",
    "    \n",
    "    L1_NORM_MEAN = 6471.3016\n",
    "    L1_NORM_STD = 1596.4753\n",
    "    \n",
    "    best_counterfactuals = []\n",
    "    feature_importance_data = []\n",
    "    \n",
    "    # Process each sample\n",
    "    for sample_idx in range(n_samples):\n",
    "        # Keep sample in fp32 during processing for numerical stability\n",
    "        sample = X_eval_tensor[sample_idx]\n",
    "        initial_pred = initial_predictions[sample_idx]\n",
    "        \n",
    "        best_sample_score = float('inf')\n",
    "        best_sample_classification = initial_pred.item()\n",
    "        best_sample_candidate = sample.clone()\n",
    "        sample_feature_importance = torch.zeros(n_features, device=device, dtype=torch.float32)\n",
    "        \n",
    "        # Process each first feature\n",
    "        for first_feat_idx in range(n_features):\n",
    "            first_samples = first_feature_samples[first_feat_idx].float()\n",
    "            \n",
    "            best_feature_score = float('inf')\n",
    "            best_feature_pred = initial_pred.item()\n",
    "            best_feature_candidate = sample.clone()\n",
    "            \n",
    "            # Process each second feature\n",
    "            for second_feat_idx, second_feat_global_idx in enumerate(second_feature_indices):\n",
    "                second_samples = second_feature_samples[second_feat_idx].float()\n",
    "                \n",
    "                # Use chunked processing or vectorized approach\n",
    "                if use_chunked_processing:\n",
    "                    score, pred, candidate = process_candidates_chunked(\n",
    "                        sample, first_feat_idx, second_feat_global_idx,\n",
    "                        first_samples, second_samples, mlp_model, osvm_model,\n",
    "                        classification_weight, distance_weight, osvm_weight,\n",
    "                        L1_NORM_MEAN, L1_NORM_STD, use_mixed_precision,\n",
    "                        chunk_size\n",
    "                    )\n",
    "                else:\n",
    "                    # Vectorized approach (faster but uses more memory)\n",
    "                    grid_first, grid_second = torch.meshgrid(first_samples, second_samples, indexing='ij')\n",
    "                    n_candidates = grid_first.numel()\n",
    "                    \n",
    "                    candidates = sample.unsqueeze(0).expand(n_candidates, -1).contiguous()\n",
    "                    candidates[:, first_feat_idx] = grid_first.flatten()\n",
    "                    candidates[:, second_feat_global_idx] = grid_second.flatten()\n",
    "                    \n",
    "                    with torch.no_grad():\n",
    "                        if use_mixed_precision:\n",
    "                            with autocast():\n",
    "                                mlp_predictions = mlp_model(candidates).squeeze()\n",
    "                        else:\n",
    "                            mlp_predictions = mlp_model(candidates).squeeze()\n",
    "                        \n",
    "                        if mlp_predictions.dim() == 0:\n",
    "                            mlp_predictions = mlp_predictions.unsqueeze(0)\n",
    "                        \n",
    "                        osvm_scores = osvm_model.decision_function(candidates)\n",
    "                        l1_distances = torch.sum(torch.abs(candidates - sample), dim=1)\n",
    "                        \n",
    "                        classification_component = mlp_predictions * classification_weight\n",
    "                        distance_component = torch.abs(l1_distances - L1_NORM_MEAN) / L1_NORM_STD * distance_weight\n",
    "                        osvm_component = osvm_scores * osvm_weight\n",
    "                        \n",
    "                        total_scores = - classification_component - distance_component + osvm_component\n",
    "                        \n",
    "                        best_idx = torch.argmax(total_scores)\n",
    "                        score = total_scores[best_idx].item()\n",
    "                        pred = mlp_predictions[best_idx].item()\n",
    "                        candidate = candidates[best_idx]\n",
    "                    \n",
    "                    # Memory cleanup\n",
    "                    del candidates, mlp_predictions, osvm_scores, l1_distances\n",
    "                    del classification_component, distance_component, osvm_component, total_scores\n",
    "                \n",
    "                # Update best for this first feature\n",
    "                if score < best_feature_score:\n",
    "                    best_feature_score = score\n",
    "                    best_feature_pred = pred\n",
    "                    best_feature_candidate = candidate\n",
    "            \n",
    "            # Update feature importance\n",
    "            sample_feature_importance[first_feat_idx] = initial_pred - best_feature_pred\n",
    "            \n",
    "            # Update global best for this sample\n",
    "            if best_feature_score < best_sample_score:\n",
    "                best_sample_score = best_feature_score\n",
    "                best_sample_classification = best_feature_pred\n",
    "                best_sample_candidate = best_feature_candidate\n",
    "            \n",
    "            # Update progress tracker\n",
    "            progress_tracker.update(sample_idx, first_feat_idx)\n",
    "            \n",
    "            # Periodic cleanup\n",
    "            if (first_feat_idx + 1) % 200 == 0:\n",
    "                torch.cuda.empty_cache()\n",
    "        \n",
    "        # Store results after processing all features for this sample\n",
    "        cf_row = [best_sample_classification] + best_sample_candidate.cpu().numpy().tolist()\n",
    "        best_counterfactuals.append(cf_row)\n",
    "        \n",
    "        importance_row = sample_feature_importance.cpu().numpy().tolist()\n",
    "        feature_importance_data.append(importance_row)\n",
    "        \n",
    "        # Update progress tracker (sample complete)\n",
    "        progress_tracker.update(sample_idx)\n",
    "        \n",
    "        # Memory cleanup after each sample\n",
    "        if (sample_idx + 1) % 20 == 0:\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Finish progress tracking\n",
    "    progress_tracker.finish()\n",
    "    \n",
    "    # Save results\n",
    "    print(\"Saving results...\")\n",
    "    cf_columns = ['classification'] + [f'feature_{i}' for i in range(n_features)]\n",
    "    importance_columns = [f'feature_{i}_diff' for i in range(n_features)]\n",
    "    \n",
    "    best_cf_df = pd.DataFrame(best_counterfactuals, columns=cf_columns)\n",
    "    importance_df = pd.DataFrame(feature_importance_data, columns=importance_columns)\n",
    "    \n",
    "    best_cf_df.to_parquet(output_cf_path, engine='pyarrow', compression='snappy', index=False)\n",
    "    importance_df.to_parquet(output_importance_path, engine='pyarrow', compression='snappy', index=False)\n",
    "    \n",
    "    print(f\"Saved counterfactuals: {output_cf_path}\")\n",
    "    print(f\"Saved feature importance: {output_importance_path}\")\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return best_cf_df, importance_df\n",
    "\n",
    "\n",
    "\n",
    "best_cfs2dim, feature_importance2dim = generate_2feature_counterfactuals_osvm_optimized(\n",
    "    data_eval_path='data_eval_norm100.parquet',\n",
    "    model_path='mlp_model.pth',\n",
    "    osvm_model_path='OSVM_TCGA_class0_20kfeatures_gpu2.pkl',\n",
    "    second_feature_json_path='top_2_percent_feature_indexes.json',\n",
    "    num_points_first=4,\n",
    "    num_points_second=6,\n",
    "    range_percent=-15,\n",
    "    classification_weight=20.0,\n",
    "    distance_weight=5.0,\n",
    "    osvm_weight=10.0,\n",
    "    gpu_id=0,\n",
    "    use_mixed_precision=True,\n",
    "    prediction_batch_size=2048,\n",
    "    use_chunked_processing=True,  # Set to True for maximum memory efficiency\n",
    "    chunk_size=1024\n",
    ")\n",
    "\n",
    "if best_cfs is not None:\n",
    "    print(f\"\\nSuccess! Generated {len(best_cfs)} counterfactuals\")\n",
    "    print(f\"Feature importance shape: {feature_importance.shape}\")\n",
    "else:\n",
    "    print(\"Generation failed - check error messages above\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
